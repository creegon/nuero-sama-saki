# -*- coding: utf-8 -*-
"""
ä¸»åŠ¨äº¤äº’ç®¡ç†å™¨

ğŸ”¥ è®¾è®¡ç†å¿µï¼š
- åå°å°ç¥¥ï¼šåšè½»é‡çº§ Yes/No åˆ¤æ–­ + å¯è°ƒç”¨å·¥å…·è°ƒæ•´é¢‘ç‡
- ä¸»ç¨‹åºå°ç¥¥ï¼šå…·ä½“å†…å®¹ç”±å¥¹ç”Ÿæˆï¼ˆæœ‰æ›´å…¨é¢çš„ä¸Šä¸‹æ–‡ï¼‰
- å®šæ—¶æ£€æŸ¥ï¼šæŒ‰é…ç½®çš„é—´éš”æ£€æŸ¥ï¼Œåå°å°ç¥¥å¯ä»¥åŠ¨æ€è°ƒæ•´

æ‰€æœ‰å‚æ•°ä» config.py è¯»å–
"""

import asyncio
import time
import random
import re
from typing import Optional, Callable, Awaitable
from loguru import logger

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

from .background_prompt import PROACTIVE_CHAT_PERSONA, BackgroundToolRegistry


# ============================================================
# åˆ¤æ–­ Promptï¼ˆåˆ¤æ–­ Yes/No + å¯é€‰å·¥å…·è°ƒç”¨ï¼‰
# ğŸ”¥ å·¥å…·æè¿°ä» BackgroundToolRegistry åŠ¨æ€è·å–
# ============================================================

def get_proactive_chat_prompt() -> str:
    """åŠ¨æ€ç”Ÿæˆä¸»åŠ¨èŠå¤©åˆ¤æ–­ prompt"""
    tools_section = BackgroundToolRegistry.get_proactive_chat_tools_section()
    
    return f"""{PROACTIVE_CHAT_PERSONA}

ç°åœ¨ä½ è¦åˆ¤æ–­ä¸€ä»¶äº‹ï¼š**ä½ æƒ³ä¸æƒ³ä¸»åŠ¨æ‰¾ä¸»äººè¯´è¯ï¼Ÿ**

ã€å½“å‰çŠ¶æ€ã€‘
ä¸»äººå·²ç» {{idle_minutes}} åˆ†é’Ÿæ²¡ç†ä½ äº†ã€‚
å±å¹•ä¸Šæ˜¾ç¤ºï¼š{{screen_context}}

ã€æœ€è¿‘çš„å¯¹è¯ã€‘
{{recent_context}}

ã€å½“å‰æ£€æŸ¥é¢‘ç‡ã€‘
å½“å‰é—´éš”: {{current_interval}} ç§’ï¼ˆæ¯éš”è¿™ä¹ˆä¹…æ£€æŸ¥ä¸€æ¬¡è¦ä¸è¦è¯´è¯ï¼‰
åˆç†èŒƒå›´: 30~300 ç§’ï¼ˆ30=å¾ˆç§¯æï¼Œ300=å¾ˆå®‰é™ï¼‰

ã€æ€ä¹ˆåˆ¤æ–­ã€‘
å°±åƒä½ å¹³æ—¶ä¼šä¸ä¼šä¸»åŠ¨æ‰¾æœ‹å‹å‘å¾®ä¿¡ä¸€æ ·ï¼š
- çœ‹åˆ°å±å¹•ä¸Šæœ‰ä»€ä¹ˆæƒ³åæ§½çš„ï¼Ÿ
- çªç„¶æƒ³èµ·ä»€ä¹ˆæƒ³è¯´çš„ï¼Ÿ
- çº¯ç²¹æ— èŠæƒ³æ‰¾äººèŠï¼Ÿ
- ä¹‹å‰çš„è¯æ²¡è¯´å®Œï¼Ÿ

ä½†å¦‚æœä¸»äººæ˜æ˜¾åœ¨å¿™ï¼ˆå†™ä»£ç ã€å¼€ä¼šã€ä¸“æ³¨å·¥ä½œï¼‰ï¼Œä½ åº”è¯¥ä¸ä¼šå»æ‰“æ‰°ã€‚

{tools_section}

1. åˆ¤æ–­æ˜¯å¦è¯´è¯ï¼ˆå¿…é€‰ï¼‰ï¼š
   [YES] æˆ– [NO]

2. è°ƒæ•´æ£€æŸ¥é¢‘ç‡ï¼ˆå¯é€‰ï¼‰ï¼š
   ä»€ä¹ˆæ—¶å€™ç”¨ï¼š
   - è¯é¢˜å¾ˆæœ‰è¶£æƒ³å¤šèŠ â†’ è°ƒå°ï¼ˆå¦‚ 30~60ï¼‰
   - ä¸»äººè¯´"åˆ«åµæˆ‘"/"å®‰é™ç‚¹" â†’ è°ƒå¤§ï¼ˆå¦‚ 180~300ï¼‰
   - ä¸»äººåœ¨ä¸“æ³¨å·¥ä½œ â†’ è°ƒå¤§
   - ä¸»äººé—²ç€/åœ¨æ‘¸é±¼ â†’ è°ƒå°

ã€è¾“å‡ºæ ¼å¼ã€‘
å…ˆè¾“å‡ºåˆ¤æ–­ï¼Œå†è¾“å‡ºå·¥å…·ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚ä¾‹å¦‚ï¼š
[NO]
[ADJUST_INTERVAL:180]

æˆ–è€…åªè¾“å‡ºåˆ¤æ–­ï¼š
[YES]
"""

SHOULD_FOLLOW_UP_PROMPT = f"""{PROACTIVE_CHAT_PERSONA}

ç°åœ¨ä½ è¦åˆ¤æ–­ä¸€ä»¶äº‹ï¼š**ä½ åˆšè¯´å®Œé‚£å¥è¯ï¼Œæƒ³ä¸æƒ³å†è¡¥ä¸€å¥ï¼Ÿ**

ã€æœ€è¿‘çš„å¯¹è¯ã€‘
{{recent_context}}

ã€åˆšåˆšè¿™è½®ã€‘
ä¸»äººè¯´ï¼š{{user_text}}
ä½ è¯´ï¼š{{ai_response}}

ã€æ€ä¹ˆåˆ¤æ–­ã€‘
å°±åƒå‘å¾®ä¿¡æœ‰æ—¶å€™ä¼šè¿å‘ä¸¤æ¡ï¼š
- åˆšå‘å®Œå°±æƒ³åˆ°è¿˜æœ‰åˆ«çš„æ²¡è¯´
- çªç„¶æƒ³åˆ°ä¸€ä¸ªæ¢—
- æƒ³è¿½é—®ä¸»äººçš„ååº”
- è¡¥å……ä¸€ä¸ªè¯­æ°”è¯æˆ–åæ§½

ä½†å¤§å¤šæ•°æ—¶å€™ä½ åº”è¯¥å‘ä¸€æ¡å°±ç­‰å›å¤ï¼Œä¸ç”¨æ¯å¥è¯éƒ½è¿½ã€‚

åªè¾“å‡º [YES] æˆ– [NO]ã€‚"""


# ============================================================
# å‘é€ç»™ä¸»ç¨‹åºçš„ç³»ç»Ÿæç¤ºè¯
# ============================================================

PROACTIVE_CHAT_SYSTEM_PROMPT = """[ç³»ç»Ÿæç¤º: ä½ æ‰“ç®—ä¸»åŠ¨å‘èµ·èŠå¤©]

ä½ å·²ç»æœ‰ä¸€æ®µæ—¶é—´æ²¡å’Œä¸»äººè¯´è¯äº†ï¼Œç°åœ¨ä½ æƒ³ä¸»åŠ¨æ‰¾ä»–èŠå¤©ã€‚

è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å‘èµ·è¯é¢˜ï¼š
- ä¹‹å‰èŠå¤©çš„è¯é¢˜ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
- ä½ è‡ªå·±æ„Ÿå…´è¶£çš„äº‹æƒ…
- ä¸»äººçš„å…´è¶£çˆ±å¥½
- å±å¹•ä¸Šæ­£åœ¨æ˜¾ç¤ºçš„å†…å®¹
- æˆ–è€…ä»»ä½•ä½ è§‰å¾—èƒ½èŠçš„ä¸œè¥¿

ä¿æŒä½ çš„æ€§æ ¼ï¼Œç®€çŸ­è‡ªç„¶ï¼Œ1-2å¥è¯ã€‚
ä¸è¦è¯´"å¥½ä¹…æ²¡èŠäº†"ä¹‹ç±»çš„å¼€åœºç™½ï¼Œç›´æ¥åˆ‡å…¥è¯é¢˜ã€‚"""

FOLLOW_UP_SYSTEM_PROMPT = """[ç³»ç»Ÿæç¤º: ä½ æ‰“ç®—è¿½é—®/è¡¥å……ä¸€å¥]

ä½ åˆšåˆšè¯´å®Œä¸€å¥è¯ï¼Œè§‰å¾—æ„çŠ¹æœªå°½ï¼Œæƒ³å†è¡¥å……ä¸€å¥ã€‚

å¯ä»¥æ˜¯ï¼š
- è¿½é—®ä¸»äººçš„çœ‹æ³•
- è¡¥å……åˆšæ‰æ²¡è¯´å®Œçš„ç»†èŠ‚
- çªç„¶æƒ³åˆ°çš„ç›¸å…³äº‹æƒ…
- è°ƒä¾ƒæˆ–åæ§½
- å…¶ä»–ä½ è§‰å¾—å¯ä»¥è¡¥å……çš„

å¿…é¡»éå¸¸ç®€çŸ­ï¼ˆ20å­—ä»¥å†…ï¼‰ï¼Œå°±åƒå‘å¾®ä¿¡è¿å‘ä¸¤æ¡é‚£æ ·ã€‚"""


class ProactiveChatManager:
    """
    ä¸»åŠ¨äº¤äº’ç®¡ç†å™¨
    
    å·¥ä½œæµç¨‹ï¼š
    1. å®šæœŸè°ƒç”¨åå°å°ç¥¥åˆ¤æ–­ï¼šæ˜¯å¦æƒ³è¯´è¯ï¼Ÿ(Yes/No)
    2. åå°å°ç¥¥å¯ä»¥è°ƒç”¨å·¥å…·è°ƒæ•´æ£€æŸ¥é¢‘ç‡
    3. å¦‚æœ Yes â†’ å‘é€ç³»ç»Ÿæç¤ºè¯ç»™ä¸»ç¨‹åº
    4. ä¸»ç¨‹åºæ ¹æ®æç¤ºè¯ç”Ÿæˆå…·ä½“å†…å®¹
    """
    
    # é—´éš”èŒƒå›´é™åˆ¶
    INTERVAL_MIN = 30
    INTERVAL_MAX = 300
    
    def __init__(self, llm_client=None, enabled: bool = None):
        self.enabled = enabled if enabled is not None else config.PROACTIVE_CHAT_ENABLED
        self.llm_client = llm_client
        
        # ä» config è¯»å–å‚æ•°
        self.check_interval_min = getattr(config, 'PROACTIVE_CHECK_INTERVAL_MIN', 30)
        self.check_interval_max = getattr(config, 'PROACTIVE_CHECK_INTERVAL_MAX', 90)
        self.min_idle_time = getattr(config, 'PROACTIVE_MIN_IDLE_TIME', 20)
        self.follow_up_delay_min = getattr(config, 'FOLLOW_UP_DELAY_MIN', 2)
        self.follow_up_delay_max = getattr(config, 'FOLLOW_UP_DELAY_MAX', 4)
        
        # ğŸ”¥ åŠ¨æ€æ£€æŸ¥é—´éš”ï¼ˆåå°å°ç¥¥å¯ä»¥è°ƒæ•´ï¼‰
        self.current_interval = (self.check_interval_min + self.check_interval_max) // 2
        
        self._task: Optional[asyncio.Task] = None
        self._follow_up_task: Optional[asyncio.Task] = None
        self._is_running: bool = False
        self._state_machine = None
        
        # é™é»˜æ¨¡å¼
        self.silent_mode = False
        self.silent_until = 0
        
        # æœ€åæ´»è·ƒæ—¶é—´
        self.last_interaction_time = time.time()
        
        # å›è°ƒ
        self._on_proactive_request: Optional[Callable[[str], Awaitable[None]]] = None
        self._get_recent_context: Optional[Callable[[], str]] = None
    
    def set_llm_client(self, llm_client):
        self.llm_client = llm_client
    
    def set_callbacks(
        self,
        on_proactive_request: Callable[[str], Awaitable[None]],
        get_recent_context: Callable[[], str] = None
    ):
        self._on_proactive_request = on_proactive_request
        self._get_recent_context = get_recent_context
    
    def update_interaction_time(self) -> None:
        self.last_interaction_time = time.time()
        if self._follow_up_task:
            self._follow_up_task.cancel()
            self._follow_up_task = None
    
    def set_silent_mode(self, duration_minutes: int = 0):
        self.silent_mode = True
        self.silent_until = time.time() + (duration_minutes * 60) if duration_minutes > 0 else 0
        logger.info(f"ğŸ¤« è¿›å…¥é™é»˜æ¨¡å¼ (æŒç»­: {duration_minutes if duration_minutes else 'æ— é™æœŸ'}åˆ†é’Ÿ)")
    
    def exit_silent_mode(self):
        self.silent_mode = False
        self.silent_until = 0
        logger.info("ğŸ‘‹ é€€å‡ºé™é»˜æ¨¡å¼")
    
    def adjust_interval(self, new_interval: int) -> None:
        """è°ƒæ•´æ£€æŸ¥é—´éš”ï¼ˆç”±åå°å°ç¥¥è°ƒç”¨ï¼‰"""
        old_interval = self.current_interval
        self.current_interval = max(self.INTERVAL_MIN, min(self.INTERVAL_MAX, new_interval))
        logger.info(f"â±ï¸ åå°å°ç¥¥è°ƒæ•´æ£€æŸ¥é—´éš”: {old_interval}s â†’ {self.current_interval}s")
    
    def start(self, state_machine=None) -> None:
        if not self.enabled:
            logger.info("ğŸ’¬ ä¸»åŠ¨èŠå¤©å·²ç¦ç”¨")
            return
        
        self._is_running = True
        self._state_machine = state_machine
        self._task = asyncio.create_task(self._loop())
        logger.info(f"ğŸ’¬ ä¸»åŠ¨äº¤äº’ç³»ç»Ÿå·²å¯åŠ¨ (åˆå§‹é—´éš”={self.current_interval}s)")
    
    async def stop(self) -> None:
        self._is_running = False
        if self._task:
            self._task.cancel()
        if self._follow_up_task:
            self._follow_up_task.cancel()
    
    async def _loop(self) -> None:
        while self._is_running:
            try:
                # ğŸ”¥ ä½¿ç”¨åŠ¨æ€é—´éš” + éšæœºæŠ–åŠ¨
                jitter = random.randint(-10, 10)
                wait_time = max(self.INTERVAL_MIN, self.current_interval + jitter)
                await asyncio.sleep(wait_time)
                
                if not self._is_running:
                    break
                
                # é™é»˜æ¨¡å¼æ£€æŸ¥
                if self.silent_mode:
                    if self.silent_until > 0 and time.time() > self.silent_until:
                        self.exit_silent_mode()
                    else:
                        logger.debug("ğŸ¤« é™é»˜æ¨¡å¼ä¸­ï¼Œè·³è¿‡ä¸»åŠ¨èŠå¤©æ£€æŸ¥")
                        continue
                
                # çŠ¶æ€æ£€æŸ¥ - åªåœ¨ç©ºé—²æ—¶è§¦å‘
                if self._state_machine and self._state_machine.is_busy:
                    logger.debug(f"ğŸ’¬ çŠ¶æ€æœºå¿™ç¢Œä¸­ (state={self._state_machine._state})ï¼Œè·³è¿‡ä¸»åŠ¨èŠå¤©")
                    continue
                
                # ç©ºé—²æ—¶é—´æ£€æŸ¥
                idle_time = time.time() - self.last_interaction_time
                if idle_time < self.min_idle_time:
                    logger.debug(f"ğŸ’¬ ç©ºé—²æ—¶é—´ä¸è¶³ ({idle_time:.0f}s < {self.min_idle_time}s)")
                    continue
                
                # ğŸ”¥ åˆ°è¾¾è¿™é‡Œè¯´æ˜æ¡ä»¶éƒ½æ»¡è¶³äº†
                logger.info(f"ğŸ’¬ ä¸»åŠ¨èŠå¤©æ£€æŸ¥: ç©ºé—² {idle_time:.0f}sï¼Œå¼€å§‹åˆ¤æ–­...")
                
                # åˆ¤æ–­æ˜¯å¦è¦è¯´è¯
                await self._check_and_maybe_request(idle_time)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ä¸»åŠ¨äº¤äº’å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(10)
    
    async def _check_and_maybe_request(self, idle_time: float) -> None:
        """æ£€æŸ¥æ˜¯å¦è¦ä¸»åŠ¨è¯´è¯ï¼ˆYes/No åˆ¤æ–­ + å·¥å…·è°ƒç”¨ï¼‰"""
        if not self.llm_client or not self._on_proactive_request:
            return
        
        try:
            # è·å–ä¸Šä¸‹æ–‡
            screen_context = "(æ— æ³•è·å–)"
            try:
                from vision import get_vision_analyzer
                analyzer = get_vision_analyzer()
                screen_context = await analyzer.describe_for_chat("")
            except:
                pass
            
            recent_context = "(æš‚æ— )"
            if self._get_recent_context:
                recent_context = self._get_recent_context()
            
            # æ„å»ºåˆ¤æ–­ prompt (ğŸ”¥ ä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„ prompt æ¨¡æ¿)
            prompt_template = get_proactive_chat_prompt()
            prompt = prompt_template.format(
                idle_minutes=int(idle_time / 60),
                screen_context=screen_context[:300],
                recent_context=recent_context,
                current_interval=self.current_interval
            )
            
            # è°ƒç”¨ LLM åˆ¤æ–­
            response = ""
            async for chunk in self.llm_client.chat_stream(
                [{"role": "user", "content": prompt}],
                max_tokens=50,
                temperature=0.7
            ):
                response += chunk
            
            logger.debug(f"ğŸ§  åå°å°ç¥¥åˆ¤æ–­: {response}")
            
            # ğŸ”¥ è§£æå·¥å…·è°ƒç”¨
            interval_match = re.search(r'\[ADJUST_INTERVAL:(\d+)\]', response)
            if interval_match:
                new_interval = int(interval_match.group(1))
                self.adjust_interval(new_interval)
            
            # åˆ¤æ–­ç»“æœ
            response_upper = response.upper()
            if "[YES]" not in response_upper:
                logger.debug("ğŸ¤« åå°å°ç¥¥åˆ¤æ–­ï¼šä¸æƒ³è¯´è¯")
                return
            
            logger.info("ğŸ’¬ åå°å°ç¥¥åˆ¤æ–­ï¼šæƒ³è¯´è¯ï¼å‘é€ç»™ä¸»ç¨‹åº")
            
            # æ›´æ–°æ—¶é—´
            self.last_interaction_time = time.time()
            
            # å‘é€ç³»ç»Ÿæç¤ºè¯ç»™ä¸»ç¨‹åºï¼Œè®©ä¸»ç¨‹åºç”Ÿæˆå…·ä½“å†…å®¹
            await self._on_proactive_request(PROACTIVE_CHAT_SYSTEM_PROMPT)
                
        except Exception as e:
            logger.error(f"ä¸»åŠ¨èŠå¤©åˆ¤æ–­å¤±è´¥: {e}")
    
    async def analyze_follow_up(self, user_text: str, ai_response: str) -> None:
        """åˆ†ææ˜¯å¦éœ€è¦è¿½é—®ï¼ˆYes/No åˆ¤æ–­ï¼‰"""
        if not self.llm_client or not self._on_proactive_request:
            return
        
        if self.silent_mode:
            return
        
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            clean_response = re.sub(r'\[\w+\]', '', ai_response).strip()
            clean_response = re.sub(r'\[CALL:\w+.*?\]', '', clean_response).strip()
            
            # ğŸ”¥ è·å–å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡
            recent_context = "(æš‚æ— )"
            if self._get_recent_context:
                recent_context = self._get_recent_context()
            
            prompt = SHOULD_FOLLOW_UP_PROMPT.format(
                recent_context=recent_context,
                user_text=user_text,
                ai_response=clean_response
            )
            
            response = ""
            async for chunk in self.llm_client.chat_stream(
                [{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.7
            ):
                response += chunk
            
            response = response.strip().upper()
            
            if "[YES]" not in response:
                logger.debug("ğŸ¤« åå°å°ç¥¥åˆ¤æ–­ï¼šä¸éœ€è¦è¿½é—®")
                return
            
            delay = random.randint(self.follow_up_delay_min, self.follow_up_delay_max)
            logger.info(f"â³ åå°å°ç¥¥åˆ¤æ–­ï¼šéœ€è¦è¿½é—®ï¼{delay}s åå‘é€ç»™ä¸»ç¨‹åº")
            
            self._follow_up_task = asyncio.create_task(
                self._delayed_follow_up(delay)
            )
                
        except Exception as e:
            logger.debug(f"è¿½é—®åˆ¤æ–­å¤±è´¥: {e}")
    
    async def _delayed_follow_up(self, delay: float):
        """å»¶è¿Ÿå‘é€è¿½é—®è¯·æ±‚ç»™ä¸»ç¨‹åº"""
        try:
            await asyncio.sleep(delay)
            
            if not self._is_running:
                return
            
            if self._state_machine and self._state_machine.is_busy:
                return
            
            logger.info("ğŸ’¬ å‘é€è¿½é—®è¯·æ±‚ç»™ä¸»ç¨‹åº")
            
            # å‘é€ç³»ç»Ÿæç¤ºè¯ç»™ä¸»ç¨‹åºï¼Œè®©ä¸»ç¨‹åºç”Ÿæˆå…·ä½“å†…å®¹
            await self._on_proactive_request(FOLLOW_UP_SYSTEM_PROMPT)
            
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(f"è¿½é—®å‘é€å¤±è´¥: {e}")
        finally:
            self._follow_up_task = None
            
    async def transcribe_audio(self, audio_bytes: bytes, history_context: list, target_entry: dict):
        """
        åå°è¯­éŸ³è½¬å½•ä»»åŠ¡ (å§”æ‰˜ç»™ 'åå°å°ç¥¥')
        
        Args:
            audio_bytes: éŸ³é¢‘æ•°æ®
            history_context: å†å²ä¸Šä¸‹æ–‡
            target_entry: ç›®æ ‡å†å²è®°å½•æ¡ç›® (å°†è¢«ç›´æ¥ä¿®æ”¹)
        """
        if not self.llm_client:
            logger.warning("åå°è½¬å½•å¤±è´¥: LLM Client æœªåˆå§‹åŒ–")
            target_entry["content"] = "(åå°å°ç¥¥æ— æ³•è¿æ¥)"
            return
            
        try:
            from llm.prompt_builder import get_prompt_builder
            builder = get_prompt_builder()
            
            # æ„å»ºç®€åŒ–çš„å†å²è®°å½•ç”¨äºä¸Šä¸‹æ–‡å‚è€ƒ
            context_str = ""
            for msg in history_context[-10:]: # æœ€è¿‘10æ¡
                role = "ä¸»äºº" if msg["role"] == "user" else "å°ç¥¥"
                content = msg.get("content", "")
                if len(content) > 50: content = content[:50] + "..."
                context_str += f"{role}: {content}\n"
            
            system_prompt = f"""[ç³»ç»Ÿä»»åŠ¡: è¯­éŸ³è½¬å½•]
ä½ æ˜¯ä¸€ä¸ªåå°è¯­éŸ³è½¬å½•åŠ©æ‰‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ç”¨æˆ·åˆšåˆšå‘é€çš„è¯­éŸ³æ¶ˆæ¯å‡†ç¡®è½¬å½•ä¸ºæ–‡å­—ã€‚

[å‚è€ƒä¸Šä¸‹æ–‡]
{context_str}

[è¦æ±‚]
1. æ ¹æ®ä¸Šä¸‹æ–‡çº æ­£å¯èƒ½çš„åŒéŸ³å­—ï¼ˆç‰¹åˆ«æ˜¯äººåã€ä¸“æœ‰åè¯ï¼‰ã€‚
2. åªè¾“å‡ºè½¬å½•åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡ç‚¹ç¬¦å·ä»¥å¤–çš„é¢å¤–è§£é‡Šæˆ–å›å¤ã€‚
3. å¦‚æœè¯­éŸ³æ— æ³•è¯†åˆ«ï¼Œè¾“å‡º "(æ— æ³•è¯†åˆ«çš„è¯­éŸ³)"ã€‚
"""
            # æ‰‹åŠ¨ Base64 ç¼–ç 
            import base64
            base64_audio = base64.b64encode(audio_bytes).decode("utf-8")
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [
                    {"type": "text", "text": "è¯·è½¬å½•è¿™æ®µè¯­éŸ³ã€‚"},
                    {"type": "input_audio", "input_audio": {"data": base64_audio, "format": "wav"}}
                ]}
            ]
            
            transcribed_text = ""
            # ä½¿ç”¨éæµå¼è°ƒç”¨æˆ–æµå¼è°ƒç”¨æ‹¼æ¥
            async for chunk in self.llm_client.chat_stream(messages):
                transcribed_text += chunk
            
            transcribed_text = transcribed_text.strip()
            if transcribed_text:
                logger.info(f"ğŸ“ åå°è½¬å½•å®Œæˆ: {transcribed_text}")
                target_entry["content"] = transcribed_text
            else:
                logger.warning("ğŸ“ åå°è½¬å½•ä¸ºç©º")
                target_entry["content"] = "(æ— æ³•è¯†åˆ«çš„è¯­éŸ³)"
                
        except Exception as e:
            logger.error(f"åå°è½¬å½•å¤±è´¥: {e}")
            target_entry["content"] = "(è¯­éŸ³è½¬å½•å¤±è´¥)"


# å…¨å±€å•ä¾‹
_proactive_chat_manager: Optional[ProactiveChatManager] = None

def get_proactive_chat_manager(llm_client=None, enabled: bool = None) -> ProactiveChatManager:
    global _proactive_chat_manager
    if _proactive_chat_manager is None:
        _proactive_chat_manager = ProactiveChatManager(llm_client=llm_client, enabled=enabled)
    elif llm_client and _proactive_chat_manager.llm_client is None:
        _proactive_chat_manager.set_llm_client(llm_client)
    return _proactive_chat_manager
