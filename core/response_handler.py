# -*- coding: utf-8 -*-
"""
å“åº”å¤„ç†å™¨

å¤„ç† LLM å“åº”ã€å·¥å…·è°ƒç”¨ã€TTS è¾“å‡ºç­‰
ä½¿ç”¨ç»„åˆæ¨¡å¼ï¼Œå§”æ‰˜ç»™å„ä¸“ç”¨å¤„ç†å™¨
"""

import asyncio
import re
from typing import Optional, Callable, List
from loguru import logger

import config
from tools.executor import get_tool_executor, ToolExecutor

# å­æ¨¡å—
from .memory_injector import get_memory_injector
from .follow_up_handler import FollowUpHandler


class ResponseHandler:
    """
    å“åº”å¤„ç†å™¨ - æ ¸å¿ƒå¯¹è¯å¤„ç†ç±»
    
    è´Ÿè´£ï¼š
    - å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆæ–‡æœ¬/éŸ³é¢‘ï¼‰
    - è°ƒç”¨ LLM ç”Ÿæˆå“åº”
    - å¤„ç†å·¥å…·è°ƒç”¨
    - ç®¡ç† TTS è¾“å‡º
    - è¿½é—®é€»è¾‘
    """
    
    def __init__(
        self,
        llm_client,
        audio_queue,
        player,
        state_machine,
        knowledge_monitor=None,
    ):
        self.llm_client = llm_client
        self.audio_queue = audio_queue
        self.player = player
        self.state_machine = state_machine
        self.knowledge_monitor = knowledge_monitor

        self.tool_executor: ToolExecutor = get_tool_executor()
        self.conversation_history: List[dict] = []
        self.current_emotion: Optional[str] = None
        self._last_retrieved_memories: List[dict] = []  # ğŸ”¥ ä¿å­˜æ£€ç´¢åˆ°çš„è®°å¿†ï¼Œä¼ ç»™åå°å°ç¥¥
        self._tool_results_this_turn: Dict[str, str] = {}  # ğŸ”¥ æœ¬è½®å·¥å…·è°ƒç”¨ç»“æœï¼Œä¾›åå°å°ç¥¥æ•´ç†

        # ğŸ”¥ æ‰“æ–­å–æ¶ˆæœºåˆ¶
        self._cancelled = False  # å–æ¶ˆæ ‡å¿—
        self._current_request_id = 0  # å½“å‰è¯·æ±‚ IDï¼Œç”¨äºåŒºåˆ†æ–°æ—§è¯·æ±‚

        # å›è°ƒ
        self._on_expression_change: Optional[Callable[[str], None]] = None
        
        # ğŸ”¥ å­æ¨¡å—
        self.memory_injector = get_memory_injector()
        # self.audio_manager å·²ç§»é™¤ï¼Œç›´æ¥ä½¿ç”¨ self.audio_queue å’Œ self.player
        
        # è¿½é—®å¤„ç†å™¨ï¼ˆéœ€è¦ TTS å¼•æ“ï¼Œå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.follow_up_handler: Optional[FollowUpHandler] = None
        self._init_follow_up_handler()
    
    def _init_follow_up_handler(self):
        """åˆå§‹åŒ–è¿½é—®å¤„ç†å™¨"""
        try:
            from tts.voxcpm_engine import get_voxcpm_engine
            tts_engine = get_voxcpm_engine()
            
            self.follow_up_handler = FollowUpHandler(
                llm_client=self.llm_client,
                audio_queue=self.audio_queue,
                player=self.player,
                tts_engine=tts_engine,
                config=config,
            )
            
            # è®¾ç½®å›è°ƒ
            self.follow_up_handler.set_callbacks(
                get_recent_context=self.get_recent_context,
                split_by_emotion=self._split_by_emotion,
                split_into_chunks=self._split_into_chunks,
                append_history=lambda entry: self.conversation_history.append(entry),
            )
        except Exception as e:
            logger.debug(f"è¿½é—®å¤„ç†å™¨åˆå§‹åŒ–è·³è¿‡: {e}")
            self.follow_up_handler = None
    
    # TTS å¼•æ“å±æ€§ï¼ˆç”¨äºè¿½é—®ï¼‰
    @property
    def tts_engine(self):
        try:
            from tts.voxcpm_engine import get_voxcpm_engine
            return get_voxcpm_engine()
        except:
            return None
    
    def set_expression_callback(self, callback: Callable[[str], None]) -> None:
        """è®¾ç½®è¡¨æƒ…å˜åŒ–å›è°ƒ"""
        self._on_expression_change = callback
    
    def cancel(self) -> None:
        """å–æ¶ˆå½“å‰å“åº”å¤„ç†ï¼ˆè¢«æ‰“æ–­æ—¶è°ƒç”¨ï¼‰"""
        self._cancelled = True
        if self.follow_up_handler:
            self.follow_up_handler.cancel()
        logger.info("ğŸ”‡ å“åº”å¤„ç†è¢«å–æ¶ˆ")
    
    def reset_cancellation(self) -> None:
        """é‡ç½®å–æ¶ˆæ ‡å¿—ï¼ˆæ–°è¯·æ±‚å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
        self._cancelled = False
        self._current_request_id += 1
        if self.follow_up_handler:
            self.follow_up_handler.reset()
        logger.debug(f"ğŸ”„ è¯·æ±‚ ID æ›´æ–°: {self._current_request_id}")
    
    async def process_user_input(self, text: str, was_interrupted: bool = False) -> None:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥
        
        Args:
            text: ç”¨æˆ·è¯´çš„è¯
            was_interrupted: æ˜¯å¦æ˜¯æ‰“æ–­åœºæ™¯
        """
        # æ£€æŸ¥ç”¨æˆ·æ„å›¾ï¼ˆé™é»˜æ¨¡å¼ç›¸å…³ï¼‰
        self._check_user_intent(text)
        
        # ğŸ”¥ å¦‚æœæ˜¯æ‰“æ–­ï¼Œæ³¨å…¥æ‰“æ–­æç¤º
        if was_interrupted:
            text = f"[ç³»ç»Ÿ: ä¸»äººæ‰“æ–­äº†ä½ è¯´è¯ï¼Œå¹¶ä¸”è¯´]\n{text}"
            logger.info("ğŸ”‡ æ‰“æ–­æç¤ºå·²æ³¨å…¥")
        
        await self._process_llm_response(text)
        self.current_emotion = None
        
        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆï¼ˆæ‰“æ–­ï¼‰
        if self._cancelled:
            logger.debug("ğŸ”‡ process_user_input: å¤„ç†å·²è¢«å–æ¶ˆï¼Œè·³è¿‡ finish_speaking")
            return
        
        self.state_machine.finish_speaking()
    
    async def process_audio_input(self, audio_data, was_interrupted: bool = False) -> None:
        """
        å¤„ç†éŸ³é¢‘è¾“å…¥ (Voice-to-LLM æ¨¡å¼)
        
        ç›´æ¥å°†éŸ³é¢‘å‘é€ç»™ LLMï¼Œè·³è¿‡ STT
        
        Args:
            audio_data: numpy array æ ¼å¼çš„éŸ³é¢‘æ•°æ®
            was_interrupted: æ˜¯å¦æ˜¯æ‰“æ–­åœºæ™¯
        """
        import numpy as np
        
        # å°† numpy éŸ³é¢‘è½¬æ¢ä¸º WAV å­—èŠ‚
        audio_bytes = self._audio_to_wav_bytes(audio_data)
        
        logger.info(f"ğŸ¤ Voice-to-LLM: å‘é€ {len(audio_bytes)//1024}KB éŸ³é¢‘")
        
        # ğŸ”¥ å¦‚æœæ˜¯æ‰“æ–­ï¼Œåœ¨æç¤ºä¸­æ³¨å…¥ä¿¡æ¯
        if was_interrupted:
            logger.info("ğŸ”‡ æ‰“æ–­åœºæ™¯ï¼šå°†æ³¨å…¥æ‰“æ–­æç¤º")
        
        await self._process_llm_response_with_audio(audio_bytes, was_interrupted=was_interrupted)
        self.current_emotion = None
        
        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆï¼ˆæ‰“æ–­ï¼‰
        if self._cancelled:
            logger.debug("ğŸ”‡ process_audio_input: å¤„ç†å·²è¢«å–æ¶ˆï¼Œè·³è¿‡ finish_speaking")
            return
        
        self.state_machine.finish_speaking()
    
    def _audio_to_wav_bytes(self, audio_data) -> bytes:
        """å°† numpy éŸ³é¢‘æ•°æ®è½¬æ¢ä¸º WAV å­—èŠ‚"""
        import io
        import wave
        import numpy as np
        
        # ç¡®ä¿æ˜¯ numpy array
        if not isinstance(audio_data, np.ndarray):
            audio_data = np.array(audio_data)
        
        # è½¬æ¢ä¸º 16-bit PCM
        if audio_data.dtype == np.float32 or audio_data.dtype == np.float64:
            audio_data = (audio_data * 32767).astype(np.int16)
        elif audio_data.dtype != np.int16:
            audio_data = audio_data.astype(np.int16)
        
        # å†™å…¥ WAV
        buffer = io.BytesIO()
        with wave.open(buffer, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(config.AUDIO_SAMPLE_RATE)
            wav_file.writeframes(audio_data.tobytes())
        
        return buffer.getvalue()
    
    async def _process_llm_response_with_audio(self, audio_bytes: bytes, was_interrupted: bool = False):
        """å¤„ç†å¸¦éŸ³é¢‘çš„ LLM å“åº” (Voice-to-LLM)"""
        from llm.prompt_builder import get_prompt_builder
        builder = get_prompt_builder()
        
        # 1. è·å– System Prompt (å¸¦ç¼“å­˜)
        system_prompt = builder.build_system_prompt()
        
        # æ‰“æ–­åœºæ™¯ï¼šæ³¨å…¥æ‰“æ–­æç¤ºåˆ° system_prompt (ä¸´æ—¶ä¿®æ”¹)
        if was_interrupted:
            system_prompt += "\n\n[ç³»ç»Ÿæç¤º: ä¸»äººåˆšåˆšæ‰“æ–­äº†ä½ è¯´è¯ï¼å¯èƒ½æœ‰äº›ç”Ÿæ°”æˆ–æ€¥äº‹ã€‚è¯·ç«‹å³åœæ­¢ä¹‹å‰çš„è¯é¢˜ï¼Œç®€çŸ­åœ°å›åº”ä¸»äººçš„æ‰“æ–­ã€‚]"
            logger.info("ğŸ”‡ æ‰“æ–­æç¤ºå·²æ³¨å…¥åˆ° system_prompt")
        
        # 2. æ„å»º User Prompt (æ–‡æœ¬éƒ¨åˆ†)
        user_text_prompt = builder.build_user_prompt(
            current_input="(è¿™æ˜¯ä¸€æ¡è¯­éŸ³æ¶ˆæ¯)",  # å ä½ç¬¦ï¼Œå®é™…å†…å®¹åœ¨éŸ³é¢‘é‡Œ
            conversation_history=self.conversation_history
        )
        
        # ğŸ” Debug: æ‰“å°å®Œæ•´ Prompt (åªåœ¨ debug çº§åˆ«)
        logger.debug("="*30 + " Prompt Debug " + "="*30)
        logger.debug(f"ã€System Promptã€‘:\n{system_prompt}")
        if was_interrupted:
            logger.debug(f"...(åŒ…å«æ‰“æ–­æç¤º)...")
        logger.debug(f"ã€User Promptã€‘:\n{user_text_prompt}")
        logger.debug("="*74)
        
        full_response = ""
        try:
            # Voice-to-LLM: æ··åˆæ¶ˆæ¯æ ¼å¼
            # æ‰‹åŠ¨ Base64 ç¼–ç éŸ³é¢‘ï¼ˆå› ä¸º chat_stream ä¸å¤„ç† bytesï¼‰
            import base64
            base64_audio = base64.b64encode(audio_bytes).decode("utf-8")
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [
                    {"type": "text", "text": user_text_prompt},
                    {"type": "input_audio", "input_audio": {"data": base64_audio, "format": "wav"}}
                ]}
            ]
            
            # ä½¿ç”¨é€šç”¨çš„ chat_streamï¼Œå› ä¸ºå®ƒæ”¯æŒç›´æ¥ä¼ é€’ messages åˆ—è¡¨
            async for chunk in self.llm_client.chat_stream(messages):
                full_response += chunk
        except Exception as e:
            logger.error(f"Voice-to-LLM å¤±è´¥: {e}")
            return
        
        if not full_response.strip():
            logger.warning("LLM å“åº”ä¸ºç©º")
            return
        
        # åŒºåˆ†ä¸åŒç±»å‹çš„å›å¤
        if system_prompt and "ä¸»åŠ¨å‘èµ·èŠå¤©" in system_prompt:
            logger.info(f"ğŸ’¬ [ä¸»åŠ¨èŠå¤©] AI: {full_response}")
        elif system_prompt and "è¿½é—®/è¡¥å……" in system_prompt:
            logger.info(f"ğŸ”„ [è¿½é—®] AI: {full_response}")
        else:
            logger.info(f"ğŸ¤– AI: {full_response}")
        
        # ğŸ”¥ æ£€æµ‹ [IGNORE] - é€‰æ‹©æ€§å“åº”
        if full_response.strip().startswith("[IGNORE]"):
            logger.info("ğŸ™ˆ AI å†³å®šå¿½ç•¥æ­¤è¾“å…¥")
            return
        
        # æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²ï¼ˆä¸ºäº†èƒ½è®©åå°è½¬å½•ä»»åŠ¡å¼•ç”¨ user_entryï¼‰
        from datetime import datetime
        timestamp_str = datetime.now().strftime("%H:%M:%S")
        
        user_entry = {
            "role": "user",
            "content": "(è¯­éŸ³è½¬å½•ä¸­...)",  # åˆå§‹å ä½ç¬¦
            "timestamp": timestamp_str
        }
        
        assistant_entry = {
            "role": "assistant",
            "content": full_response,
            "timestamp": timestamp_str
        }
        
        self.conversation_history.append(user_entry)
        self.conversation_history.append(assistant_entry)
        
        # ğŸš€ å¯åŠ¨åå°è½¬å½•ä»»åŠ¡ (å§”æ‰˜ç»™ 'åå°å°ç¥¥' - ProactiveChatManager)
        # æ³¨æ„ï¼šuser_entry æ˜¯å¼•ç”¨ä¼ é€’ï¼Œåå°ä»»åŠ¡ä¼šç›´æ¥ä¿®æ”¹å®ƒçš„ content
        history_snapshot = list(self.conversation_history[:-2])
        
        from core.proactive_chat import get_proactive_chat_manager
        manager = get_proactive_chat_manager()
        asyncio.create_task(
            manager.transcribe_audio(audio_bytes, history_snapshot, user_entry)
        )

        # æ£€æµ‹å·¥å…·è°ƒç”¨
        tool_match = re.search(r'\[CALL:(\w+)(?::([^\]]*))?\]', full_response)
        if tool_match:
            await self._handle_tool_call("[è¯­éŸ³è¾“å…¥]", full_response, loop_count=0, append_history=False)
        else:
            # ä¼ é€’ append_history=Falseï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨æ·»åŠ äº†
            await self._speak_response(full_response, "[è¯­éŸ³è¾“å…¥]", append_history=False)
    
    async def _process_llm_response(
        self,
        user_text: str,
        tool_result: str = None,
        tool_name: str = None,
        loop_count: int = 0
    ) -> None:
        """å¤„ç† LLM å“åº” (æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨)"""

        # æ„å»ºæ¶ˆæ¯
        if tool_result:
            messages = self._build_messages(user_text)
            tool_display_name = {
                "screenshot": "å±å¹•å†…å®¹",
                "screenshot_describe": "å±å¹•æè¿°",
                "knowledge": "è®°å¿†æœç´¢",
                "web_search": "ç½‘ç»œæœç´¢"
            }.get(tool_name, "å·¥å…·")
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡ç»“æœ (IMAGE_RESULT:æ ¼å¼:base64æ•°æ®)
            if tool_result.startswith("IMAGE_RESULT:"):
                # è§£æå›¾ç‰‡æ•°æ®
                parts = tool_result.split(":", 2)
                if len(parts) == 3:
                    image_format = parts[1]  # jpeg or png
                    base64_data = parts[2]
                    
                    # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼Œè®© LLM ç›´æ¥"çœ‹"å›¾ç‰‡
                    messages.append({
                        "role": "user",
                        "content": [
                            {"type": "text", "text": f"[ç³»ç»Ÿ: è¿™æ˜¯ä¸»äººçš„å±å¹•æˆªå›¾]\n\nè¯·ä»”ç»†çœ‹è¿™å¼ å›¾ç‰‡ï¼Œæè¿°ä½ çœ‹åˆ°äº†ä»€ä¹ˆï¼Œç„¶åç”¨ä½ çš„è¯­æ°”å›ç­”ä¸»äººã€‚"},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/{image_format};base64,{base64_data}"
                                }
                            }
                        ]
                    })
                    logger.info(f"ğŸ“¸ å‘é€å›¾ç‰‡ç»™ LLM (æ ¼å¼: {image_format})")
                else:
                    # æ ¼å¼é”™è¯¯ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬å¤„ç†
                    messages.append({
                        "role": "user",
                        "content": f"[ç³»ç»Ÿ: {tool_display_name}ç»“æœ]\n{tool_result}"
                    })
            else:
                # æ™®é€šæ–‡æœ¬å·¥å…·ç»“æœ
                messages.append({
                    "role": "user",
                    "content": f"[ç³»ç»Ÿ: {tool_display_name}ç»“æœ]\n{tool_result}\n\nç°åœ¨è¯·åŸºäºè¿™ä¸ªç»“æœï¼Œç”¨ä½ çš„è¯­æ°”è‡ªç„¶åœ°å›ç­”ä¸»äººçš„é—®é¢˜ã€‚å¦‚æœä½ éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥ç»§ç»­è°ƒç”¨å…¶ä»–å·¥å…·ã€‚è®°å¾—ä¿æŒè§’è‰²æ€§æ ¼ï¼Œä¸è¦åƒå®¢æœæˆ–æœºå™¨äººã€‚"
                })
        else:
            messages = self._build_messages(user_text)
        
        # è·å– LLM å“åº”ï¼ˆæ–°æ¶æ„ï¼šsystem prompt å·²åœ¨ messages ä¸­ï¼‰
        full_response = ""
        print("ğŸ¤– AI: ", end="", flush=True)
        async for chunk in self.llm_client.chat_stream(messages):
            full_response += chunk
            print(chunk, end="", flush=True)
        print()
        
        # æ£€æµ‹ [IGNORE] - é€‰æ‹©æ€§å“åº”
        if full_response.strip().startswith("[IGNORE]"):
            logger.info("ğŸ™ˆ AI å†³å®šå¿½ç•¥æ­¤è¾“å…¥")
            return
        
        # æ£€æµ‹å·¥å…·è°ƒç”¨
        # ç¡®å®šç”¨äºä¸‹ä¸€è½®/å†å²è®°å½•çš„æ–‡æœ¬
        # å¦‚æœæ˜¯å·¥å…·ç»“æœå›åˆï¼Œå†å²è®°å½•åº”è¯¥æ˜¾ç¤ºå·¥å…·ç»“æœï¼ˆæˆ–å…¶æ‘˜è¦ï¼‰ï¼Œè€Œä¸æ˜¯é‡å¤åŸå§‹ç”¨æˆ·è¾“å…¥
        history_text = user_text
        if tool_result:
            tool_display_name = {
                "screenshot": "å±å¹•æˆªå›¾",
                "screenshot_describe": "å±å¹•æè¿°",
                "knowledge": "è®°å¿†æœç´¢",
                "web_search": "ç½‘ç»œæœç´¢",
                "add_knowledge": "è®°å¿†æ·»åŠ ",
                "move_self": "ç§»åŠ¨ä¸æ§åˆ¶"
            }.get(tool_name, tool_name)
            
            if tool_result.startswith("IMAGE_RESULT:"):
                history_text = f"[ç³»ç»Ÿ: {tool_display_name} (å›¾ç‰‡)]"
            else:
                # æˆªæ–­è¿‡é•¿çš„å·¥å…·ç»“æœç”¨äºæ˜¾ç¤º
                display_result = tool_result[:50] + "..." if len(tool_result) > 50 else tool_result
                history_text = f"[ç³»ç»Ÿ: {tool_display_name}ç»“æœ] {display_result}"

        if self.tool_executor.has_tool_call(full_response):
            await self._handle_tool_call(history_text, full_response, loop_count)
        else:
            await self._speak_response(full_response, history_text)
    
    async def _handle_tool_call(self, user_text: str, response: str, loop_count: int = 0, append_history: bool = True) -> None:
        """å¤„ç†å·¥å…·è°ƒç”¨ (å§”æ‰˜ç»™ ToolExecutor)"""
        
        MAX_TOOL_LOOPS = 5
        if loop_count >= MAX_TOOL_LOOPS:
            logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å·¥å…·å¾ªç¯æ¬¡æ•° ({MAX_TOOL_LOOPS})ï¼Œå¼ºåˆ¶ç»“æŸ")
            return
            
        tool_result, tool_name, after_text = await self.tool_executor.handle_tool_execution(
            response=response,
            user_text=user_text,
            conversation_history=self.conversation_history if append_history else [], # å¦‚æœä¸è¿½åŠ å†å²ï¼Œä¼ ç©ºåˆ—è¡¨æˆ–æ ¹æ®é€»è¾‘è°ƒæ•´
            # Callbacks
            on_speak=lambda text, emotion: self.audio_queue.submit(text, emotion),
            on_play_audio=self._play_audio_queue,
            on_expression=self._set_expression,
            is_speaking_check=lambda: self.state_machine.is_speaking,
            start_speaking_call=self.state_machine.start_speaking,
            # Monitors
            knowledge_monitor=self.knowledge_monitor,
            memory_helper=self.memory_injector,  # Executor still expects 'memory_helper' arg, passing injector is compatible if interface matches? 
            # Wait, executor might rely on strict typing or specific methods? 
            # Executor uses: memory_helper.search_raw_memories
            # Injector now has this method. So it's fine.
            last_retrieved_memories=self._last_retrieved_memories
        )
        
        # å¦‚æœ append_history ä¸º Falseï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±ç®¡ç†å†å²? 
        # executor å†…éƒ¨ä¼š append åˆ°ä¼ å…¥çš„ listã€‚å¦‚æœä¼ å…¥ self.conversation_historyï¼Œå®ƒå°±ä¼š appendã€‚
        # å¦‚æœ append_history=Falseï¼Œæˆ‘ä»¬ä¼ ä¸€ä¸ªä¸´æ—¶ list é¿å…æ±¡æŸ“ä¸»å†å²? Or just let executor handle it?
        # åŸé€»è¾‘ï¼šå¦‚æœ append_history=True æ‰ appendã€‚
        # æ‰€ä»¥ä¸Šé¢ conversation_history å‚æ•°ä¼ é€’å–å†³äº append_historyã€‚
        # è¿™é‡Œ: conversation_history=self.conversation_history if append_history else [] æ˜¯ä¸å¤Ÿçš„ï¼Œ
        # å› ä¸º executor å†…éƒ¨æ˜¯ç›´æ¥ appendã€‚å¦‚æœä¼ ç©º listï¼ŒåŸæ¥ append_history=True çš„é€»è¾‘å°±æ²¡æ‰§è¡Œåˆ° self.conversation_historyã€‚
        # ç­‰ç­‰ï¼Œexecutor.handle_tool_execution åªæœ‰åœ¨ append_history=True æ—¶æ‰åº”è¯¥ append å—ï¼Ÿ
        # æ˜¯çš„ã€‚æ‰€ä»¥åº”è¯¥ä¼ ï¼š
        # conversation_history=self.conversation_history if append_history else [] 
        # è¿™æ · executor ä¼š append åˆ°è¿™ä¸ª list (æ— è®ºæ˜¯çœŸçš„è¿˜æ˜¯å‡çš„)ã€‚å¦‚æœæ˜¯å‡çš„ï¼Œå°±ä¸¢å¼ƒäº†ã€‚ç¬¦åˆé¢„æœŸã€‚

        if not tool_name:
            return
        
        # ğŸ”¥ æ”¶é›†å·¥å…·ç»“æœï¼ˆä¾›åå°å°ç¥¥æ•´ç†ï¼‰
        if tool_result and not tool_result.startswith("IMAGE_RESULT:"):
            self._tool_results_this_turn[tool_name] = tool_result[:500]  # é™åˆ¶é•¿åº¦

        # ğŸ”¥ å¤„ç†å·¥å…·è°ƒç”¨åé¢çš„æ–‡æœ¬
        if after_text:
            clean_after = re.sub(r'\s+', '', after_text.strip())
            if clean_after:
                logger.info(f"ğŸ“¢ æ’­æ”¾å·¥å…·è°ƒç”¨åçš„æ–‡æœ¬: {clean_after[:30]}...")
                after_segments = self._split_by_emotion(after_text)
                for emotion, text in after_segments:
                    if text:
                        self.audio_queue.submit(text, emotion)
                await self._play_audio_queue()

        await self._process_llm_response(user_text, tool_result, tool_name, loop_count + 1)
    
    def _check_user_intent(self, user_text: str) -> None:
        """æ£€æŸ¥ç”¨æˆ·æ„å›¾ï¼ˆé™é»˜/å”¤é†’æ¨¡å¼ï¼‰"""
        from core.proactive_chat import get_proactive_chat_manager
        manager = get_proactive_chat_manager()
        
        # é™é»˜æ¨¡å¼å…³é”®è¯
        silent_keywords = ["åˆ«åµ", "å®‰é™", "é—­å˜´", "å»å¿™", "ä¸èŠäº†", "å…ˆä¸èŠ"]
        if any(kw in user_text for kw in silent_keywords):
            manager.set_silent_mode(duration_minutes=60)
            return
        
        # å”¤é†’å…³é”®è¯
        wake_keywords = ["èŠèŠ", "è¯´è¯", "åœ¨å—", "å‡ºæ¥", "å¿™å®Œäº†"]
        if any(kw in user_text for kw in wake_keywords):
            manager.exit_silent_mode()
            return

    async def _speak_response(self, response: str, user_text: str, append_history: bool = True) -> None:
        """
        å¤„ç†æ™®é€šå“åº” - æ”¯æŒåŠ¨æ€è¡¨æƒ…å’Œæ™ºèƒ½åˆ†æ®µ
        
        é€»è¾‘ï¼š
        1. æŒ‰è¡¨æƒ…æ ‡ç­¾å°†å“åº”åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ ([(emo1, text1), (emo2, text2)...])
        2. å¯¹æ¯ä¸ªç‰‡æ®µï¼Œå¦‚æœæœ‰å·¥å…·è°ƒç”¨åˆ™ç§»é™¤
        3. å¯¹æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œä½¿ç”¨æ™ºèƒ½åˆ†æ®µ (_split_into_chunks) æ§åˆ¶é•¿åº¦ï¼ˆé¿å…é•¿å¥TTSå´©åï¼‰
        4. ä¾æ¬¡æäº¤ç»™éŸ³é¢‘é˜Ÿåˆ—ï¼Œå®ç°"è¯»åˆ°å“ªé‡Œå˜ä»€ä¹ˆè¡¨æƒ…"
        """
        
        # 1. æŒ‰è¡¨æƒ…æ ‡ç­¾åˆ‡åˆ†
        segments = self._split_by_emotion(response)
        if not segments:
            logger.warning("å“åº”ä¸ºç©ºæˆ–æ— æ³•è§£æ")
            return
        
        total_chunks = 0
        
        for emotion, text in segments:
            # 2. æ¸…ç†å·¥å…·è°ƒç”¨ (EmotionParser å¯èƒ½å·²ç»å¤„ç†äº†ä¸€éƒ¨åˆ†ï¼Œè¿™é‡Œç¡®ä¿å¹²å‡€)
            text = self.tool_executor.remove_tool_calls(text).strip()
            
            if not text:
                continue
                
            # 3. æ™ºèƒ½åˆ†æ®µ (æ§åˆ¶å•å¥é•¿åº¦ï¼Œé¿å… VoxCPM é•¿å¥å´©å)
            # ä½¿ç”¨ä¹‹å‰å®ç°çš„ _split_into_chunks (60å­—é˜ˆå€¼ + å¥å­ç»“æŸç¬¦åˆ‡åˆ†)
            sub_chunks = self._split_into_chunks(text)
            
            # 4. æäº¤åˆ†æ®µ
            for chunk in sub_chunks:
                if chunk:
                    self.audio_queue.submit(chunk, emotion)
                    total_chunks += 1
        
        if total_chunks > 0:
            logger.info(f"ğŸ’¾ TTS: å…±æäº¤ {total_chunks} ä¸ªç‰‡æ®µ (æ”¯æŒåŠ¨æ€è¡¨æƒ…)")
        else:
            logger.warning("æ¸…ç†åæ— æœ‰æ•ˆæ–‡æœ¬å¯è¯»")
        
        
        # è®°å½•å¯¹è¯å†å²ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        if append_history:
            from datetime import datetime
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.conversation_history.append({
                "role": "user", 
                "content": user_text,
                "timestamp": timestamp
            })
            self.conversation_history.append({
                "role": "assistant", 
                "content": response,
                "timestamp": timestamp
            })

        if len(self.conversation_history) > 30:  # è§¦å‘æ‘˜è¦
            # ğŸ”¥ ä½¿ç”¨ ConversationSummarizer æ‘˜è¦æ—§æ¶ˆæ¯
            try:
                from core.conversation_summarizer import get_conversation_summarizer
                summarizer = get_conversation_summarizer(self.llm_client)
                if summarizer:
                    # å¼‚æ­¥æ‘˜è¦å¹¶æˆªæ–­
                    asyncio.create_task(self._summarize_and_truncate())
                else:
                    # æ²¡æœ‰æ‘˜è¦å™¨æ—¶ç›´æ¥æˆªæ–­
                    self.conversation_history = self.conversation_history[-30:]
            except Exception as e:
                logger.debug(f"æ‘˜è¦å™¨è°ƒç”¨å¤±è´¥: {e}")
                self.conversation_history = self.conversation_history[-30:]

        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆï¼ˆæ‰“æ–­ï¼‰
        if self._cancelled:
            logger.debug("ğŸ”‡ _speak_response: å¤„ç†å·²è¢«å–æ¶ˆï¼Œè·³è¿‡åç»­æ‰§è¡Œ")
            return
        
        if not self.state_machine.is_speaking:
            self.state_machine.start_speaking()
        
        # ğŸ”¥ å¹¶è¡Œå¤„ç†ï¼šåœ¨æ’­æ”¾éŸ³é¢‘çš„åŒæ—¶å¯åŠ¨è¿½é—®åˆ¤æ–­
        # è¿½é—®å†…å®¹ç”Ÿæˆåä¼šç›´æ¥è¿½åŠ åˆ°éŸ³é¢‘é˜Ÿåˆ—
        follow_up_task = None
        clean_text = re.sub(r'\[\w+\]', '', response)
        clean_text = self.tool_executor.remove_tool_calls(clean_text)
        if "[IGNORE]" not in response:
            try:
                from core.proactive_chat import get_proactive_chat_manager
                manager = get_proactive_chat_manager()
                if manager.llm_client:
                    # ğŸ”¥ å¯åŠ¨è¿½é—®åˆ¤æ–­ï¼ˆå¹¶è¡Œï¼‰ï¼Œä¼ å…¥ response_handler ä»¥ä¾¿è¿½åŠ åˆ°é˜Ÿåˆ—
                    follow_up_task = asyncio.create_task(
                        self._handle_follow_up(user_text, clean_text)
                    )
            except Exception as e:
                logger.debug(f"Follow-up åˆ†æè·³è¿‡: {e}")
        
        # æ’­æ”¾éŸ³é¢‘é˜Ÿåˆ—
        await self._play_audio_queue()
        
        # ğŸ”¥ ç­‰å¾…è¿½é—®ä»»åŠ¡å®Œæˆï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if follow_up_task:
            try:
                await follow_up_task
            except asyncio.CancelledError:
                pass

        # é€šçŸ¥çŸ¥è¯†ç›‘æ§å™¨ï¼ˆåå°å°ç¥¥ï¼‰
        if self.knowledge_monitor:
            # ğŸ”¥ æ£€ç´¢å½“å‰å¯¹è¯ç›¸å…³çš„åŸå§‹è®°å¿†ï¼ˆåŒ…å« IDï¼‰
            # ğŸ”¥ æ£€æŸ¥ user_text æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯å ä½ç¬¦ï¼‰
            if not self._last_retrieved_memories and user_text and user_text not in ["[è¯­éŸ³è¾“å…¥]", ""]:
                self._last_retrieved_memories = self.memory_injector.search_raw_memories(user_text, n_results=5)
            
            asyncio.create_task(
                self.knowledge_monitor.analyze_conversation(
                    user_text, response, self._last_retrieved_memories
                )
            )
        
        # ğŸ”¥ åå°æ•´ç†å·¥å…·è°ƒç”¨ç»“æœï¼ˆä¾›ä¸‹è½®å¯¹è¯ä½¿ç”¨ï¼‰
        if self._tool_results_this_turn:
            try:
                from core.context_manager import get_context_manager
                context_manager = get_context_manager(self.llm_client)
                
                # æ„å»ºå¯¹è¯æ‘˜è¦
                clean_response = re.sub(r'\[\w+\]', '', response).strip()
                conversation = f"ä¸»äºº: {user_text}\nå°ç¥¥: {clean_response}"
                
                # åå°å¼‚æ­¥æ•´ç†ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                asyncio.create_task(
                    context_manager.prepare_context(conversation, self._tool_results_this_turn)
                )
                logger.debug(f"ğŸ“‹ å¯åŠ¨å·¥å…·ç»“æœæ•´ç† ({len(self._tool_results_this_turn)} ä¸ªç»“æœ)")
            except Exception as e:
                logger.debug(f"å·¥å…·ç»“æœæ•´ç†å¯åŠ¨å¤±è´¥: {e}")
            finally:
                # æ¸…ç©ºæœ¬è½®ç»“æœ
                self._tool_results_this_turn = {}
    
    async def _summarize_and_truncate(self):
        """æ‘˜è¦æ—§å¯¹è¯å¹¶æˆªæ–­å†å²"""
        try:
            from core.conversation_summarizer import get_conversation_summarizer
            summarizer = get_conversation_summarizer(self.llm_client)
            if summarizer:
                self.conversation_history = await summarizer.check_and_summarize(
                    self.conversation_history,
                    threshold=30,
                    keep_recent=10
                )
        except Exception as e:
            logger.error(f"å¯¹è¯æ‘˜è¦å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ç›´æ¥æˆªæ–­
            if len(self.conversation_history) > 30:
                self.conversation_history = self.conversation_history[-30:]
    
    def _split_by_emotion(self, text: str) -> list:
        """æŒ‰æƒ…ç»ªæ ‡ç­¾åˆ†æ®µï¼ˆå§”æ‰˜ç»™ EmotionParserï¼‰"""
        from core.emotion_parser import get_emotion_parser
        parser = get_emotion_parser(self.tool_executor)
        return parser.split_by_emotion(text)
    
    def _build_messages(self, user_text: str) -> List[dict]:
        """
        æ„å»ºå¯¹è¯æ¶ˆæ¯ï¼ˆæ–°æ¶æ„ï¼Œå‚è€ƒ MaiBotï¼‰
        
        ä½¿ç”¨ PromptBuilder æ„å»ºï¼š
        - System: è§’è‰²è®¾å®š + è®°å¿†ï¼ˆå¸¦ç¼“å­˜ï¼Œä¸æ¯è½®è°ƒç”¨ï¼‰
        - User: ç®€æ´å¯¹è¯å†å² + å½“å‰è¾“å…¥
        """
        from llm.prompt_builder import get_prompt_builder
        
        builder = get_prompt_builder()
        
        # ä½¿ç”¨æ–°æ¶æ„ï¼šåªè¿”å› system + user ä¸¤æ¡æ¶ˆæ¯
        # è®°å¿†åœ¨ system prompt ä¸­ä¸€æ¬¡æ€§æ³¨å…¥ï¼Œä¸æ¯è½®è°ƒç”¨
        messages = builder.build_messages(
            current_input=user_text,
            conversation_history=self.conversation_history
        )
        
        # ğŸ” Debug: æ‰“å°å®Œæ•´ Prompt (ä¿®æ”¹ä¸º info çº§åˆ«ä»¥ä¾¿æŸ¥çœ‹)
        logger.info("="*30 + " Prompt Debug " + "="*30)
        logger.info(f"ã€System Promptã€‘:\n{messages[0]['content']}")
        logger.info(f"ã€User Promptã€‘:\n{messages[1]['content']}")
        logger.info("="*74)
        
        return messages
    
    def _get_system_context(self) -> str:
        """è·å–ç³»ç»Ÿä¸Šä¸‹æ–‡"""
        return self.memory_injector.get_system_context()
    
    def _get_recent_memories(self, n: int = 5) -> str:
        """è·å–æœ€è¿‘è®°å¿†ï¼ˆè‡ªåŠ¨æ³¨å…¥åˆ°å¯¹è¯ä¸­ï¼‰"""
        return self.memory_injector.get_recent_memories(n=n)
    
    def _get_important_memories(self) -> str:
        """è·å–æ ¸å¿ƒå±‚è®°å¿†ï¼ˆé«˜é‡è¦æ€§ï¼Œå§‹ç»ˆæ³¨å…¥ï¼‰"""
        return self.memory_injector.get_important_memories()
    
    async def _handle_follow_up(self, user_text: str, ai_response: str) -> None:
        """ğŸ”¥ å¤„ç†è¿½é—®ï¼šå§”æ‰˜ç»™ FollowUpHandler"""
        if self.follow_up_handler:
            self.follow_up_handler._cancelled = self._cancelled
            await self.follow_up_handler.handle_follow_up(user_text, ai_response)
    
    def _search_related_memories(self, query: str) -> str:
        """æœç´¢ç›¸å…³è®°å¿†"""
        return self.memory_injector.search_related_memories(query)
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºé€‚åˆ TTS çš„å°æ®µ
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä¿æŒæ•´æ®µï¼ˆè¿è´¯æ€§æœ€å¥½ï¼‰
        2. å¦‚æœè¶…è¿‡ 60 å­—ï¼Œå°è¯•åœ¨å¥å­ç»“æŸç¬¦ï¼ˆã€‚.ï¼ï¼Ÿ!?ï¼‰å¤„åˆ†å‰²ï¼ˆé¿å…é•¿å¥å¯¼è‡´çš„è¯­é€Ÿ/éŸ³è°ƒé—®é¢˜ï¼‰
        3. å¦‚æœå¿…é¡»åˆ†å‰²ï¼Œå°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´
        """
        if not text:
            return []
            
        # é¢„å¤„ç†ï¼šå»é™¤å¤šä½™ç©ºç™½
        text = text.strip()
        length = len(text)
        
        # 1. å¦‚æœçŸ­äº 60 å­—ï¼Œç›´æ¥æ•´æ®µè¿”å›
        if length <= 60:
            return [text]
            
        logger.info(f"æ–‡æœ¬è¾ƒé•¿ ({length} å­—)ï¼Œæ‰§è¡Œæ™ºèƒ½åˆ†æ®µ...")
        chunks = []
        current_chunk = ""
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­ï¼Œä¿ç•™åˆ†éš”ç¬¦
        # ([ã€‚.ï¼ï¼Ÿ!?]+) åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç»“æŸç¬¦
        import re
        parts = re.split(r'([ã€‚.ï¼ï¼Ÿ!?]+)', text)
        
        # re.split ä¿ç•™åˆ†éš”ç¬¦æ—¶ï¼Œåˆ—è¡¨ä¼šæ˜¯ [å¥1, åˆ†éš”ç¬¦1, å¥2, åˆ†éš”ç¬¦2, ...]
        # æˆ‘ä»¬éœ€è¦ä¸¤ä¸¤åˆå¹¶ï¼šå¥1+åˆ†éš”ç¬¦1
        sentences = []
        for i in range(0, len(parts) - 1, 2):
            sentences.append(parts[i] + parts[i+1])
        if len(parts) % 2 != 0 and parts[-1]: # å¤„ç†æœ€åå¯èƒ½æ²¡æœ‰åˆ†éš”ç¬¦çš„éƒ¨åˆ†
            sentences.append(parts[-1])
            
        for sentence in sentences:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°å¥å­è¶…è¿‡ 60 å­—ï¼Œä¸”å½“å‰å—ä¸ä¸ºç©ºï¼Œåˆ™å…ˆæäº¤å½“å‰å—
            if len(current_chunk) + len(sentence) > 60 and current_chunk:
                chunks.append(current_chunk)
                current_chunk = sentence
            else:
                current_chunk += sentence
                
        if current_chunk:
            chunks.append(current_chunk)
            
        logger.info(f"åˆ†æ®µç»“æœ: {len(chunks)} æ®µ")
        return chunks
    
    async def _play_audio_queue(self) -> None:
        """æ’­æ”¾ TTS é˜Ÿåˆ—ï¼ˆæ”¯æŒæ‰“æ–­ï¼‰"""
        while self.audio_queue.has_pending():
            # æ£€æŸ¥æ‰“æ–­
            if self.audio_queue.is_interrupted or self._cancelled:
                logger.info("ğŸ”‡ æ£€æµ‹åˆ°æ‰“æ–­ï¼Œåœæ­¢æ’­æ”¾")
                return
            await asyncio.sleep(0.1)
            task = self.audio_queue.get_next_ready()
            if task and (task.audio_data or task.audio_path):
                source = task.audio_data if task.audio_data else task.audio_path
                self.player.add(task.id, source, task.text)
        
        # ç­‰å¾…æ’­æ”¾å®Œæˆ
        while self.player.is_playing:
            if self.audio_queue.is_interrupted or self._cancelled:
                logger.info("ğŸ”‡ æ£€æµ‹åˆ°æ‰“æ–­ï¼Œåœæ­¢æ’­æ”¾")
                self.player.clear()
                return
            await asyncio.sleep(0.1)
    
    def _set_expression(self, emotion: str) -> None:
        """è®¾ç½®è¡¨æƒ…"""
        if self._on_expression_change:
            self._on_expression_change(emotion)
        logger.info(f"ğŸ­ è¡¨æƒ…: {emotion}")
    
    def get_recent_context(self) -> str:
        """è·å–å®Œæ•´å¯¹è¯å†å²ï¼ˆç”¨äºåå°å°ç¥¥åˆ¤æ–­ï¼‰"""
        if not self.conversation_history:
            return "ï¼ˆè¿˜æ²¡æœ‰å¯¹è¯ï¼‰"
        
        # ğŸ”¥ ç›´æ¥è¿”å›å®Œæ•´å¯¹è¯å†å²ï¼Œä¸åšæˆªå–
        lines = []
        for msg in self.conversation_history:
            role = "ä¸»äºº" if msg["role"] == "user" else "ä½ "
            lines.append(f"{role}: {msg['content']}")
        
        return "\n".join(lines)
