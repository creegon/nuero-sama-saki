# -*- coding: utf-8 -*-
"""
è¿½é—®å¤„ç†å™¨

ä» ResponseHandler æå–çš„è¿½é—®é€»è¾‘
"""

import asyncio
import random
import re
from typing import Callable, List, Optional
import sys
import os

# æ·»åŠ æ ¹ç›®å½•åˆ°pathä»¥å¯¼å…¥config
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

from loguru import logger


class FollowUpHandler:
    """è¿½é—®å¤„ç†å™¨ - å¤„ç†è¿½é—®åˆ¤æ–­ã€ç”Ÿæˆå’Œæ’­æ”¾"""
    
    def __init__(
        self,
        llm_client,
        audio_queue,
        player,
        tts_engine=None,
        config=None,
    ):
        self.llm_client = llm_client
        self.audio_queue = audio_queue
        self.player = player
        self.tts_engine = tts_engine
        self.config = config
        
        # å–æ¶ˆæ ‡å¿—ï¼ˆç”±å¤–éƒ¨è®¾ç½®ï¼‰
        self._cancelled = False
        
        # å›è°ƒå‡½æ•°
        self._get_recent_context: Optional[Callable[[], str]] = None
        self._split_by_emotion: Optional[Callable[[str], List[tuple]]] = None
        self._split_into_chunks: Optional[Callable[[str], List[str]]] = None
        self._append_history: Optional[Callable[[dict], None]] = None
    
    def set_callbacks(
        self,
        get_recent_context: Callable[[], str] = None,
        split_by_emotion: Callable[[str], List[tuple]] = None,
        split_into_chunks: Callable[[str], List[str]] = None,
        append_history: Callable[[dict], None] = None,
    ):
        """è®¾ç½®å›è°ƒå‡½æ•°"""
        if get_recent_context:
            self._get_recent_context = get_recent_context
        if split_by_emotion:
            self._split_by_emotion = split_by_emotion
        if split_into_chunks:
            self._split_into_chunks = split_into_chunks
        if append_history:
            self._append_history = append_history
    
    def cancel(self):
        """å–æ¶ˆè¿½é—®"""
        self._cancelled = True
    
    def reset(self):
        """é‡ç½®å–æ¶ˆæ ‡å¿—"""
        self._cancelled = False
    
    async def handle_follow_up(self, user_text: str, ai_response: str) -> None:
        """
        ğŸ”¥ å¤„ç†è¿½é—®ï¼šå¹¶è¡Œåˆ¤æ–­ + ç”Ÿæˆ + è¿½åŠ åˆ°éŸ³é¢‘é˜Ÿåˆ—
        
        ä¸éŸ³é¢‘æ’­æ”¾å¹¶è¡Œæ‰§è¡Œï¼Œè¿½é—®å†…å®¹ç”Ÿæˆåç›´æ¥è¿½åŠ åˆ°é˜Ÿåˆ—
        """
        try:
            from core.proactive_chat import get_proactive_chat_manager
            manager = get_proactive_chat_manager()
            
            if not manager.llm_client or manager.silent_mode:
                return
            
            # ğŸ”¥ è·å–å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡
            recent_context = "(æš‚æ— )"
            if self._get_recent_context:
                recent_context = self._get_recent_context()
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿½é—®ï¼ˆè°ƒç”¨åå°å°ç¥¥ï¼‰
            from core.proactive_chat import SHOULD_FOLLOW_UP_PROMPT
            
            clean_response = re.sub(r'\[\w+\]', '', ai_response).strip()
            clean_response = re.sub(r'\[CALL:\w+.*?\]', '', clean_response).strip()
            
            prompt = SHOULD_FOLLOW_UP_PROMPT.format(
                recent_context=recent_context,
                user_text=user_text,
                ai_response=clean_response
            )
            
            response = ""
            async for chunk in manager.llm_client.chat_stream(
                [{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.7
            ):
                response += chunk
            
            response = response.strip().upper()
            
            if "[YES]" not in response:
                logger.debug("ğŸ¤« åå°å°ç¥¥åˆ¤æ–­ï¼šä¸éœ€è¦è¿½é—®")
                return
            
            
            logger.info("ğŸ’¬ åå°å°ç¥¥åˆ¤æ–­ï¼šéœ€è¦è¿½é—®ï¼ç”Ÿæˆè¿½é—®å†…å®¹...")
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¢«æ‰“æ–­
            if self._cancelled:
                logger.debug("ğŸ”‡ è¿½é—®ç”Ÿæˆå·²å–æ¶ˆï¼ˆè¢«æ‰“æ–­ï¼‰")
                return
            
            # ğŸ”¥ ç­‰å¾…ä¸»å›å¤çš„éŸ³é¢‘æ’­æ”¾å®Œæ¯•
            logger.debug("â³ ç­‰å¾…ä¸»å›å¤éŸ³é¢‘æ’­æ”¾å®Œæ¯•...")
            while self.audio_queue.has_pending() or self.player.is_playing:
                if self._cancelled or self.audio_queue.is_interrupted:
                    logger.debug("ğŸ”‡ è¿½é—®ç­‰å¾…æœŸé—´è¢«æ‰“æ–­")
                    return
                await asyncio.sleep(0.1)
            
            # ğŸ”¥ ä¸»å›å¤æ’­æ”¾å®Œæ¯•åï¼Œå¼€å§‹å»¶è¿Ÿè®¡æ—¶
            delay = random.randint(
                getattr(config, 'FOLLOW_UP_DELAY_MIN', 2),
                getattr(config, 'FOLLOW_UP_DELAY_MAX', 4)
            )
            logger.info(f"â³ è¿½é—®å»¶è¿Ÿ {delay}s...ï¼ˆä¸»å›å¤å·²æ’­æ”¾å®Œæ¯•ï¼‰")
            await asyncio.sleep(delay)
            
            # ğŸ”¥ å†æ¬¡æ£€æŸ¥æ˜¯å¦è¢«æ‰“æ–­
            if self._cancelled:
                logger.debug("ğŸ”‡ è¿½é—®å·²å–æ¶ˆï¼ˆå»¶è¿ŸæœŸé—´è¢«æ‰“æ–­ï¼‰")
                return
            
            # ğŸ”¥ è°ƒç”¨ä¸»ç¨‹åºå°ç¥¥ç”Ÿæˆè¿½é—®å†…å®¹
            # ä½¿ç”¨ FOLLOW_UP_SYSTEM_PROMPT ä½œä¸ºç³»ç»Ÿæç¤º
            from core.proactive_chat import FOLLOW_UP_SYSTEM_PROMPT
            from llm.prompt_builder import get_prompt_builder
            
            builder = get_prompt_builder()
            
            # æ„å»ºæ¶ˆæ¯ï¼šsystem prompt + å®Œæ•´å¯¹è¯å†å²
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ä¸»ç¨‹åºçš„ llm_client å’Œå®Œæ•´ä¸Šä¸‹æ–‡
            messages = builder.build_messages(
                current_input=FOLLOW_UP_SYSTEM_PROMPT,  # è¿½é—®æç¤ºä½œä¸ºè¾“å…¥
                conversation_history=[]  # ä¸éœ€è¦å†å²ï¼Œå› ä¸º system prompt å·²ç»åŒ…å«
            )
            
            # è¦†ç›–system messageï¼ŒåŠ å…¥è¿½é—®æç¤º
            messages[0]['content'] = f"""{messages[0]['content']}

{FOLLOW_UP_SYSTEM_PROMPT}

ã€å‚è€ƒä¿¡æ¯ã€‘
æœ€è¿‘å¯¹è¯ï¼š{recent_context}"""
            
            follow_up_response = ""
            async for chunk in self.llm_client.chat_stream(
                messages,
                max_tokens=100,  # é™åˆ¶é•¿åº¦ï¼Œè¿½é—®åº”è¯¥ç®€çŸ­
                temperature=0.8
            ):
                follow_up_response += chunk
            
            if not follow_up_response.strip():
                return
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¢«æ‰“æ–­
            if self._cancelled:
                logger.debug("ğŸ”‡ è¿½é—®ç”Ÿæˆå·²å–æ¶ˆï¼ˆè¢«æ‰“æ–­ï¼‰")
                return
            
            logger.info(f"ğŸ”„ [è¿½é—®] AI: {follow_up_response}")
            
            # ğŸ”¥ å¤„ç†å“åº”ï¼šæå–æƒ…ç»ªã€æ¸…ç†æ–‡æœ¬ã€æäº¤TTS
            segments = []
            if self._split_by_emotion:
                segments = self._split_by_emotion(follow_up_response)
            if not segments:
                return
            
            # æå–ç¬¬ä¸€ä¸ªè¡¨æƒ…
            initial_emotion = segments[0][0]
            
            # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤æ‰€æœ‰è¡¨æƒ…æ ‡ç­¾å’Œå·¥å…·è°ƒç”¨
            clean_text = re.sub(r'\[[a-zA-Z_]+\]', '', follow_up_response)
            from tools.executor import get_tool_executor
            executor = get_tool_executor()
            clean_text = executor.remove_tool_calls(clean_text)
            clean_text = clean_text.strip()
            
            if not clean_text:
                return
            
            # æ•´æ®µæäº¤ï¼ˆå’Œresponse_handlerä¸€è‡´ï¼‰
            self.audio_queue.submit(clean_text, emotion=initial_emotion)
            logger.info(f"ğŸ’¾ è¿½é—®TTS: æ•´æ®µæäº¤ ({len(clean_text)} å­—)")
            
            # è¿½åŠ åˆ°å¯¹è¯å†å²
            if self._append_history:
                from datetime import datetime
                timestamp = datetime.now().strftime("%H:%M:%S")
                self._append_history({
                    "role": "assistant",
                    "content": follow_up_response,
                    "timestamp": timestamp
                })
            
        except asyncio.CancelledError:
            logger.debug("ğŸ”‡ è¿½é—®ä»»åŠ¡è¢«å–æ¶ˆ")
        except Exception as e:
            logger.error(f"è¿½é—®å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
