# -*- coding: utf-8 -*-
"""
å†…å­˜æ¸…ç†å·¥å…·
"""

import gc
from loguru import logger


def cleanup_all(aggressive: bool = False):
    """
    æ‰§è¡Œå†…å­˜æ¸…ç†
    
    Args:
        aggressive: æ˜¯å¦æ¿€è¿›æ¸…ç†ï¼ˆå¤šè½®ï¼‰
    """
    try:
        import torch
        
        gc.collect()
        
        if torch.cuda.is_available():
            if aggressive:
                # æ¿€è¿›æ¨¡å¼ï¼šå¤šè½®æ¸…ç†
                for i in range(3):
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    gc.collect()
                logger.info("ğŸ§¹ CUDA æ¿€è¿›æ¸…ç†å®Œæˆ (3è½®)")
            else:
                torch.cuda.empty_cache()
                logger.debug("ğŸ§¹ CUDA ç¼“å­˜å·²æ¸…ç†")
            
            torch.cuda.reset_peak_memory_stats()
        else:
            gc.collect()
            logger.debug("ğŸ§¹ Python GC å·²æ‰§è¡Œ")
            
    except Exception as e:
        logger.warning(f"æ¸…ç†å¼‚å¸¸: {e}")


def get_memory_stats() -> dict:
    """
    è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        dict: {"cuda": {...}, "ram": {...}}
    """
    stats = {"cuda": None, "ram": None}
    
    try:
        import torch
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            stats["cuda"] = {
                "allocated_gb": allocated,
                "reserved_gb": reserved
            }
    except Exception:
        pass
    
    try:
        import psutil
        mem = psutil.virtual_memory()
        stats["ram"] = {
            "used_gb": mem.used / (1024**3),
            "total_gb": mem.total / (1024**3),
            "percent": mem.percent
        }
    except Exception:
        pass
    
    return stats
