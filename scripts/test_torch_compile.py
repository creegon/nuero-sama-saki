# -*- coding: utf-8 -*-
"""
VoxCPM torch.compile åŠ é€Ÿæµ‹è¯•è„šæœ¬
éš”ç¦»æµ‹è¯• torch.compile å¯¹ VoxCPM çš„åŠ é€Ÿæ•ˆæœå’Œç¨³å®šæ€§

âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯ç‹¬ç«‹æµ‹è¯•è„šæœ¬ï¼Œä¸ä¼šå½±å“ä¸»ç¨‹åº
"""

import os
import sys
import time
import warnings
import io

# ä¿®å¤ Windows ç»ˆç«¯ç¼–ç é—®é¢˜
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np

# ============ æµ‹è¯•é…ç½® ============
TEST_TEXT = "ä½ å¥½å‘€ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œæˆ‘ä»¬å‡ºå»æ•£æ•£æ­¥å§ï¼Ÿ"
TEST_STEPS = 15  # ä½¿ç”¨å·²çŸ¥èƒ½å®æ—¶çš„ steps
TEST_CFG = 3.0
NUM_WARMUP = 2
NUM_RUNS = 3

# torch.compile é…ç½®é€‰é¡¹
COMPILE_BACKENDS = {
    "none": None,           # ä¸ä½¿ç”¨ compileï¼ˆåŸºå‡†ï¼‰
    "inductor": "inductor", # é»˜è®¤åç«¯ï¼Œé€šå¸¸æœ€å¿«
    "eager": "eager",       # ä¸ä¼˜åŒ–ï¼Œç”¨äºå¯¹æ¯”
}


def load_model_vanilla():
    """åŠ è½½åŸç‰ˆæ¨¡å‹ï¼ˆä¸ä½¿ç”¨ torch.compileï¼‰"""
    from voxcpm.core import VoxCPM
    
    print("\nğŸ“¦ åŠ è½½ VoxCPM 1.5 (åŸç‰ˆ)...")
    
    voxcpm = VoxCPM.from_pretrained(
        hf_model_id="openbmb/VoxCPM1.5",
        load_denoiser=False,
        optimize=False,  # å…³é—­å†…ç½®ä¼˜åŒ–
        lora_config=None,
        lora_weights_path=None,
    )
    
    return voxcpm


def apply_torch_compile(model, backend: str):
    """å¯¹æ¨¡å‹åº”ç”¨ torch.compile"""
    if backend is None:
        return model
    
    print(f"ğŸ”§ åº”ç”¨ torch.compile (backend={backend})...")
    
    try:
        # åªç¼–è¯‘ tts_model çš„ forward æ–¹æ³•
        model.tts_model = torch.compile(
            model.tts_model,
            backend=backend,
            mode="reduce-overhead",  # å‡å°‘å¼€é”€ï¼Œé€‚åˆå®æ—¶æ¨ç†
            fullgraph=False,  # å…è®¸å›¾æ–­å¼€ï¼Œæé«˜å…¼å®¹æ€§
        )
        print(f"âœ“ torch.compile å·²åº”ç”¨ (backend={backend})")
    except Exception as e:
        print(f"âŒ torch.compile å¤±è´¥: {e}")
        return None
    
    return model


def test_rtf(model, text: str, steps: int, cfg: float, sample_rate: int) -> dict:
    """æµ‹è¯•å•æ¬¡ç”Ÿæˆçš„ RTF"""
    torch.cuda.synchronize()
    start = time.perf_counter()
    
    chunks = []
    first_chunk_time = None
    
    try:
        for i, chunk in enumerate(model.generate_streaming(
            text=text,
            prompt_wav_path=None,
            prompt_text=None,
            cfg_value=cfg,
            inference_timesteps=steps,
            max_len=2048,
        )):
            if i == 0:
                torch.cuda.synchronize()
                first_chunk_time = time.perf_counter() - start
            chunks.append(chunk)
    except Exception as e:
        return {"error": str(e)}
    
    torch.cuda.synchronize()
    total_time = time.perf_counter() - start
    
    if not chunks:
        return {"error": "No audio generated"}
    
    full_wav = np.concatenate(chunks)
    audio_duration = len(full_wav) / sample_rate
    rtf = total_time / audio_duration if audio_duration > 0 else 0
    
    return {
        "rtf": rtf,
        "first_chunk_ms": first_chunk_time * 1000 if first_chunk_time else 0,
        "total_time": total_time,
        "audio_duration": audio_duration,
    }


def run_single_backend_test(backend_name: str, backend_value):
    """æµ‹è¯•å•ä¸ªåç«¯"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æµ‹è¯•åç«¯: {backend_name}")
    print(f"{'='*60}")
    
    # æ¯æ¬¡æµ‹è¯•éƒ½é‡æ–°åŠ è½½æ¨¡å‹ï¼Œç¡®ä¿éš”ç¦»
    model = load_model_vanilla()
    sample_rate = model.tts_model.sample_rate
    
    # åº”ç”¨ torch.compileï¼ˆå¦‚æœéœ€è¦ï¼‰
    if backend_value is not None:
        model = apply_torch_compile(model, backend_value)
        if model is None:
            print(f"âŒ {backend_name} åç«¯å¤±è´¥ï¼Œè·³è¿‡")
            return None
    
    # é¢„çƒ­
    print(f"\nğŸ”¥ é¢„çƒ­ ({NUM_WARMUP} æ¬¡)...")
    for i in range(NUM_WARMUP):
        result = test_rtf(model, TEST_TEXT, TEST_STEPS, TEST_CFG, sample_rate)
        if "error" in result:
            print(f"âš ï¸ é¢„çƒ­å¤±è´¥: {result['error']}")
            return None
        print(f"  é¢„çƒ­ {i+1}: RTF={result['rtf']:.2f}")
    
    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()
    
    # æ­£å¼æµ‹è¯•
    print(f"\nğŸ“ˆ æ­£å¼æµ‹è¯• ({NUM_RUNS} æ¬¡)...")
    rtfs = []
    first_chunks = []
    
    for run in range(NUM_RUNS):
        result = test_rtf(model, TEST_TEXT, TEST_STEPS, TEST_CFG, sample_rate)
        if "error" in result:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result['error']}")
            return None
        rtfs.append(result["rtf"])
        first_chunks.append(result["first_chunk_ms"])
        print(f"  Run {run+1}: RTF={result['rtf']:.2f}, é¦–åŒ…={result['first_chunk_ms']:.0f}ms")
        torch.cuda.empty_cache()
    
    avg_rtf = sum(rtfs) / len(rtfs)
    avg_first_chunk = sum(first_chunks) / len(first_chunks)
    
    # æ¸…ç†æ¨¡å‹
    del model
    torch.cuda.empty_cache()
    
    return {
        "backend": backend_name,
        "avg_rtf": avg_rtf,
        "avg_first_chunk_ms": avg_first_chunk,
        "rtfs": rtfs,
    }


def run_benchmark():
    """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
    print("=" * 60)
    print("VoxCPM torch.compile åŠ é€Ÿæµ‹è¯•")
    print("=" * 60)
    
    # æ‰“å°ç¯å¢ƒä¿¡æ¯
    print(f"\nğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    print(f"  - PyTorch: {torch.__version__}")
    print(f"  - CUDA: {torch.version.cuda}")
    print(f"  - GPU: {torch.cuda.get_device_name(0)}")
    
    # æ£€æŸ¥ triton
    try:
        import triton
        print(f"  - Triton: {triton.__version__} âœ“")
    except ImportError:
        print(f"  - Triton: âŒ æœªå®‰è£…")
        print("âš ï¸ å»ºè®®å®‰è£… triton-windows ä»¥è·å¾—æœ€ä½³ torch.compile æ€§èƒ½")
    
    print(f"\nğŸ“ æµ‹è¯•å‚æ•°:")
    print(f"  - æ–‡æœ¬: '{TEST_TEXT}'")
    print(f"  - Steps: {TEST_STEPS}")
    print(f"  - CFG: {TEST_CFG}")
    
    # æµ‹è¯•å„åç«¯
    results = []
    
    # å…ˆæµ‹è¯•åŸºå‡†ï¼ˆæ—  compileï¼‰
    result = run_single_backend_test("none", None)
    if result:
        results.append(result)
        baseline_rtf = result["avg_rtf"]
    else:
        print("âŒ åŸºå‡†æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # æµ‹è¯• inductor åç«¯
    result = run_single_backend_test("inductor", "inductor")
    if result:
        results.append(result)
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    for r in results:
        speedup = (baseline_rtf / r["avg_rtf"] - 1) * 100 if r["avg_rtf"] > 0 else 0
        status = "ğŸš€" if speedup > 5 else ("âœ…" if r["avg_rtf"] < 1.0 else "âš ï¸")
        print(f"{status} {r['backend']:10s}: RTF={r['avg_rtf']:.2f}, é¦–åŒ…={r['avg_first_chunk_ms']:.0f}ms, åŠ é€Ÿ={speedup:+.1f}%")
    
    # æ¨è
    print("\nğŸ“Œ ç»“è®º:")
    if len(results) >= 2:
        inductor = next((r for r in results if r["backend"] == "inductor"), None)
        if inductor:
            speedup = (baseline_rtf / inductor["avg_rtf"] - 1) * 100
            if speedup > 10:
                print(f"âœ… torch.compile (inductor) æœ‰æ•ˆï¼ŒåŠ é€Ÿ {speedup:.1f}%ï¼Œå»ºè®®å¯ç”¨")
            elif speedup > 0:
                print(f"âš ï¸ torch.compile (inductor) ç•¥æœ‰æå‡ ({speedup:.1f}%)ï¼Œå¯é€‰æ‹©æ€§å¯ç”¨")
            else:
                print(f"âŒ torch.compile (inductor) æ— æ•ˆç”šè‡³å˜æ…¢ï¼Œä¸å»ºè®®å¯ç”¨")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»™å‡ºå®Œæ•´å»ºè®®")
    
    print("\nâœ“ æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    # å¿½ç•¥ä¸€äº›è­¦å‘Š
    warnings.filterwarnings("ignore", category=UserWarning)
    
    run_benchmark()
