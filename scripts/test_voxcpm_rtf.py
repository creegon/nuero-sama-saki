# -*- coding: utf-8 -*-
"""
VoxCPM RTF æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„ RTFï¼Œæ‰¾åˆ°æœ€ä¼˜å‚æ•°ç»„åˆ
"""

import os
import sys
import time

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨ import torch ä¹‹å‰ï¼‰
# æµ‹è¯•ä¸åŒçš„ CUDA å†…å­˜é…ç½®
CUDA_ALLOC_CONFIGS = {
    "default": "",
    "max_split_128": "max_split_size_mb:128",
    "expandable": "expandable_segments:True",
}

# é€‰æ‹©è¦æµ‹è¯•çš„é…ç½®
CUDA_CONFIG = "expandable"  # æ”¹è¿™é‡Œæµ‹è¯•ä¸åŒé…ç½®
if CUDA_ALLOC_CONFIGS[CUDA_CONFIG]:
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = CUDA_ALLOC_CONFIGS[CUDA_CONFIG]
    print(f"âœ“ CUDA å†…å­˜é…ç½®: {CUDA_ALLOC_CONFIGS[CUDA_CONFIG]}")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from voxcpm.core import VoxCPM

# ============ æµ‹è¯•é…ç½® ============
TEST_TEXT = "ä½ å¥½å‘€ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œæˆ‘ä»¬å‡ºå»æ•£æ•£æ­¥å§ï¼Ÿ"  # ~20å­—
TEST_STEPS_LIST = [15, 20, 25, 30, 35]  # æµ‹è¯•ä¸åŒ steps
TEST_CFG_LIST = [2.5, 3.0, 3.5]  # æµ‹è¯•ä¸åŒ CFG
NUM_WARMUP = 1  # é¢„çƒ­æ¬¡æ•°
NUM_RUNS = 3    # æ¯ç»„æµ‹è¯•æ¬¡æ•°


def load_model():
    """åŠ è½½æ¨¡å‹ï¼ˆä¸åŠ è½½ LoRAï¼Œçº¯æµ‹è¯•åŸºç¡€æ€§èƒ½ï¼‰"""
    print("\nğŸ“¦ åŠ è½½ VoxCPM 1.5...")
    
    voxcpm = VoxCPM.from_pretrained(
        hf_model_id="openbmb/VoxCPM1.5",
        load_denoiser=False,  # å…³é—­é™å™ª
        optimize=False,       # å…³é—­ torch.compileï¼ˆé¿å…é¦–æ¬¡ç¼–è¯‘å»¶è¿Ÿï¼‰
        lora_config=None,
        lora_weights_path=None,
    )
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"  - é‡‡æ ·ç‡: {voxcpm.tts_model.sample_rate} Hz")
    print(f"  - è®¾å¤‡: {next(voxcpm.tts_model.parameters()).device}")
    print(f"  - dtype: {next(voxcpm.tts_model.parameters()).dtype}")
    
    return voxcpm


def test_rtf(model, text: str, steps: int, cfg: float, sample_rate: int) -> dict:
    """æµ‹è¯•å•æ¬¡ç”Ÿæˆçš„ RTF"""
    torch.cuda.synchronize()
    start = time.perf_counter()
    
    # æ”¶é›†æ‰€æœ‰ chunks
    chunks = []
    first_chunk_time = None
    
    for i, chunk in enumerate(model.generate_streaming(
        text=text,
        prompt_wav_path=None,
        prompt_text=None,
        cfg_value=cfg,
        inference_timesteps=steps,
        max_len=2048,
    )):
        if i == 0:
            torch.cuda.synchronize()
            first_chunk_time = time.perf_counter() - start
        chunks.append(chunk)
    
    torch.cuda.synchronize()
    total_time = time.perf_counter() - start
    
    # è®¡ç®—éŸ³é¢‘æ—¶é•¿
    full_wav = np.concatenate(chunks)
    audio_duration = len(full_wav) / sample_rate
    
    rtf = total_time / audio_duration if audio_duration > 0 else 0
    
    return {
        "rtf": rtf,
        "first_chunk_ms": first_chunk_time * 1000 if first_chunk_time else 0,
        "total_time": total_time,
        "audio_duration": audio_duration,
        "num_chunks": len(chunks),
    }


def run_benchmark():
    """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
    print("=" * 60)
    print("VoxCPM RTF æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # æ‰“å°ç¯å¢ƒä¿¡æ¯
    print(f"\nğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    print(f"  - PyTorch: {torch.__version__}")
    print(f"  - CUDA: {torch.version.cuda}")
    print(f"  - GPU: {torch.cuda.get_device_name(0)}")
    print(f"  - CUDA å†…å­˜é…ç½®: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF', 'é»˜è®¤')}")
    
    # åŠ è½½æ¨¡å‹
    model = load_model()
    sample_rate = model.tts_model.sample_rate
    
    # é¢„çƒ­
    print(f"\nğŸ”¥ é¢„çƒ­ ({NUM_WARMUP} æ¬¡)...")
    for _ in range(NUM_WARMUP):
        test_rtf(model, TEST_TEXT, steps=20, cfg=3.0, sample_rate=sample_rate)
    print("âœ“ é¢„çƒ­å®Œæˆ")
    
    # æ¸…ç† CUDA ç¼“å­˜
    torch.cuda.empty_cache()
    
    # è¿è¡Œæµ‹è¯•
    print(f"\nğŸ“ˆ å¼€å§‹æµ‹è¯• (æ¯ç»„ {NUM_RUNS} æ¬¡)...")
    print(f"æµ‹è¯•æ–‡æœ¬: '{TEST_TEXT}' ({len(TEST_TEXT)} å­—)")
    print("-" * 60)
    
    results = []
    
    for steps in TEST_STEPS_LIST:
        for cfg in TEST_CFG_LIST:
            rtfs = []
            first_chunks = []
            
            for run in range(NUM_RUNS):
                result = test_rtf(model, TEST_TEXT, steps, cfg, sample_rate)
                rtfs.append(result["rtf"])
                first_chunks.append(result["first_chunk_ms"])
                
                # æ¸…ç†ç¼“å­˜
                torch.cuda.empty_cache()
            
            avg_rtf = sum(rtfs) / len(rtfs)
            avg_first_chunk = sum(first_chunks) / len(first_chunks)
            
            status = "âœ…" if avg_rtf < 1.0 else ("âš ï¸" if avg_rtf < 1.5 else "âŒ")
            
            print(f"{status} Steps={steps:2d}, CFG={cfg:.1f} â†’ RTF={avg_rtf:.2f}, é¦–åŒ…={avg_first_chunk:.0f}ms")
            
            results.append({
                "steps": steps,
                "cfg": cfg,
                "avg_rtf": avg_rtf,
                "avg_first_chunk_ms": avg_first_chunk,
            })
    
    # æ‰¾åˆ°æœ€ä¼˜ç»„åˆ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    # æŒ‰ RTF æ’åº
    results.sort(key=lambda x: x["avg_rtf"])
    
    print("\nğŸ† RTF æœ€ä¼˜ Top 3:")
    for i, r in enumerate(results[:3]):
        print(f"  {i+1}. Steps={r['steps']}, CFG={r['cfg']:.1f} â†’ RTF={r['avg_rtf']:.2f}")
    
    # å®æ—¶å¯ç”¨çš„é…ç½®
    realtime = [r for r in results if r["avg_rtf"] < 1.0]
    if realtime:
        print(f"\nâœ… å¯å®æ—¶çš„é…ç½® (RTF < 1.0):")
        for r in realtime:
            print(f"  - Steps={r['steps']}, CFG={r['cfg']:.1f} â†’ RTF={r['avg_rtf']:.2f}")
    else:
        print("\nâš ï¸ æ²¡æœ‰é…ç½®èƒ½è¾¾åˆ°å®æ—¶ (RTF < 1.0)")
        print("å»ºè®®ï¼šè€ƒè™‘å®‰è£… triton æˆ–ä½¿ç”¨ torch.compile åŠ é€Ÿ")
    
    print("\nâœ“ æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    run_benchmark()
