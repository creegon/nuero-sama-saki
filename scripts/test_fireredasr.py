# -*- coding: utf-8 -*-
"""
FireRedASR-AED æµ‹è¯•è„šæœ¬

FireRedASR-AED-L (~1.1B å‚æ•°) åœ¨ä¸­æ–‡ benchmark ä¸Šè¾¾åˆ° ~3.18% CER
è¶…è¿‡ FunASR/Paraformer ä¼ ç»Ÿæ¨¡å‹

ä½¿ç”¨å‰è¯·å…ˆ:
1. git clone https://github.com/FireRedTeam/FireRedASR.git modules/FireRedASR
2. pip install -r requirements.txt (ä» FireRedASR ç›®å½•)
3. ä¸‹è½½æ¨¡å‹: huggingface-cli download FireRedTeam/FireRedASR-AED-L --local-dir modules/FireRedASR/pretrained_models/FireRedASR-AED-L
"""

# ============================================================
# Windows å…¼å®¹æ€§ä¿®å¤: å¿…é¡»åœ¨ PyTorch ä¹‹å‰å…ˆå¯¼å…¥ sentencepiece
# å¦åˆ™ä¼šå‘ç”Ÿ DLL å†²çªå¯¼è‡´ access violation å´©æºƒ
# ============================================================
import sentencepiece as _spm_preload
print("[DEBUG] sentencepiece é¢„åŠ è½½å®Œæˆ (é¿å… DLL å†²çª)")

import os
import sys
import time
import wave
import threading
import numpy as np
import sounddevice as sd
from pathlib import Path
from queue import Queue

# æ³¨æ„: ä¸å¯ç”¨ faulthandlerï¼Œå®ƒå¯èƒ½å¹²æ‰° C æ‰©å±•çš„å†…å­˜æ“ä½œ
# import faulthandler
# faulthandler.enable()

# æ³¨å†Œé€€å‡ºå¤„ç†å™¨ï¼Œæ•è·æ„å¤–é€€å‡º
import atexit
def on_exit():
    print("\n[DEBUG] atexit: è„šæœ¬æ­£åœ¨é€€å‡º...")
    sys.stdout.flush()
atexit.register(on_exit)

# é…ç½®
SAMPLE_RATE = 16000
CHUNK_SECONDS = 0.5  # æ¯ä¸ªåˆ†æ®µé•¿åº¦ (ä» 2.0 é™åˆ° 0.5 ç§’ï¼Œæ›´å¿«å“åº”)
SILENCE_THRESHOLD = 100  # é™éŸ³é˜ˆå€¼ (æ ¹æ®å®é™…éº¦å…‹é£è°ƒæ•´)
SILENCE_CHUNKS = 2  # è¿ç»­é™éŸ³å—æ•°è®¤ä¸ºè¯´è¯ç»“æŸ (0.5s * 2 = 1ç§’é™éŸ³è§¦å‘)

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# FireRedASR è·¯å¾„ (å¿…é¡»åœ¨ import fireredasr ä¹‹å‰æ·»åŠ )
FIREREDASR_PATH = os.path.join(PROJECT_ROOT, "modules", "FireRedASR")
if os.path.exists(FIREREDASR_PATH):
    sys.path.insert(0, FIREREDASR_PATH)
    print(f"å·²æ·»åŠ  FireRedASR è·¯å¾„: {FIREREDASR_PATH}")

# æ¨¡å‹ç›®å½• (åœ¨ FireRedASR æ¨¡å—å†…)
MODEL_DIR = os.path.join(PROJECT_ROOT, "modules", "FireRedASR", "pretrained_models", "FireRedASR-AED-L")


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    print("æ£€æŸ¥ä¾èµ–...")
    
    try:
        from fireredasr.models.fireredasr import FireRedAsr
        print("âœ“ FireRedASR å·²å®‰è£…")
        return True
    except ImportError as e:
        print(f"âœ— FireRedASR æœªå®‰è£…: {e}")
        print("\nå®‰è£…æ­¥éª¤:")
        print("1. git clone https://github.com/FireRedTeam/FireRedASR.git modules/FireRedASR")
        print("2. cd modules/FireRedASR && pip install -r requirements.txt")
        print("3. huggingface-cli download FireRedTeam/FireRedASR-AED-L --local-dir modules/FireRedASR/pretrained_models/FireRedASR-AED-L")
        return False


def save_temp_wav(audio: np.ndarray, path: str = "temp_test.wav"):
    """ä¿å­˜ä¸´æ—¶ WAV æ–‡ä»¶"""
    with wave.open(path, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)  # 16-bit
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(audio.tobytes())
    
    return path


def is_silence(audio: np.ndarray, threshold: int = SILENCE_THRESHOLD) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºé™éŸ³"""
    return np.abs(audio).mean() < threshold


class StreamingRecognizer:
    """æµå¼è¯­éŸ³è¯†åˆ«å™¨ - è¯´å®Œä¸€å¥å°±è¾“å‡ºä¸€å¥"""
    
    def __init__(self, model):
        self.model = model
        self.audio_queue = Queue()
        self.is_running = False
        self.current_audio = []
        self.silence_count = 0
        
    def audio_callback(self, indata, frames, time_info, status):
        """éŸ³é¢‘å›è°ƒ - å®æ—¶è·å–éº¦å…‹é£æ•°æ®"""
        if status:
            print(f"çŠ¶æ€: {status}")
        audio_chunk = indata[:, 0].copy().astype(np.int16)
        self.audio_queue.put(audio_chunk)
    
    def process_audio(self):
        """å¤„ç†éŸ³é¢‘æµ"""
        chunk_samples = int(CHUNK_SECONDS * SAMPLE_RATE)
        buffer = np.array([], dtype=np.int16)
        
        while self.is_running:
            # è·å–éŸ³é¢‘æ•°æ®
            try:
                chunk = self.audio_queue.get(timeout=0.1)
                buffer = np.concatenate([buffer, chunk])
            except:
                continue
            
            # å½“ç¼“å†²åŒºè¶³å¤Ÿå¤§æ—¶å¤„ç†
            if len(buffer) >= chunk_samples:
                audio_segment = buffer[:chunk_samples]
                buffer = buffer[chunk_samples:]
                
                # æ£€æŸ¥æ˜¯å¦é™éŸ³
                avg_volume = np.abs(audio_segment).mean()
                # è°ƒè¯•ï¼šæ˜¾ç¤ºå®é™…éŸ³é‡
                print(f"\r[DEBUG] éŸ³é‡: {avg_volume:.0f} (é˜ˆå€¼: {SILENCE_THRESHOLD})", end="", flush=True)
                
                if is_silence(audio_segment):
                    self.silence_count += 1
                    
                    # å¦‚æœä¹‹å‰æœ‰ç§¯ç´¯çš„éŸ³é¢‘ä¸”è¿ç»­é™éŸ³ï¼Œåˆ™è¯†åˆ«
                    if len(self.current_audio) > 0 and self.silence_count >= SILENCE_CHUNKS:
                        self._recognize_and_output()
                        self.silence_count = 0
                else:
                    # æœ‰å£°éŸ³ï¼Œç´¯ç§¯éŸ³é¢‘
                    self.silence_count = 0
                    self.current_audio.append(audio_segment)
                    
                    # æ˜¾ç¤ºéŸ³é‡æŒ‡ç¤º
                    volume = int(avg_volume / 100)
                    print(f"\rğŸ¤ {'â–ˆ' * min(volume, 30):<30} ({avg_volume:.0f})", end="", flush=True)
    
    def _recognize_and_output(self):
        """è¯†åˆ«å¹¶è¾“å‡ºç»“æœ"""
        if not self.current_audio:
            return
            
        # åˆå¹¶éŸ³é¢‘
        full_audio = np.concatenate(self.current_audio)
        self.current_audio = []
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_path = save_temp_wav(full_audio)
        
        # è¯†åˆ«
        try:
            print("\n\nğŸ”„ è¯†åˆ«ä¸­...", end=" ", flush=True)
            start_time = time.time()
            
            results = self.model.transcribe(
                ["stream_utterance"],
                [temp_path],
                {
                    "use_gpu": 1,
                    "beam_size": 1,
                    "nbest": 1,
                    "decode_max_len": 0,
                    "softmax_smoothing": 1.0,
                    "aed_length_penalty": 0.0,
                    "eos_penalty": 1.0
                }
            )
            
            infer_time = time.time() - start_time
            
            if results and len(results) > 0:
                text = results[0].get("text", "") if isinstance(results[0], dict) else str(results[0])
                if text.strip():
                    print(f"\n{'='*50}")
                    print(f"ğŸ“ è¯†åˆ«ç»“æœ: {text}")
                    print(f"â±ï¸  è€—æ—¶: {infer_time:.2f}s")
                    print(f"{'='*50}\n")
        except Exception as e:
            print(f"\nè¯†åˆ«é”™è¯¯: {e}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.remove(temp_path)
    
    def start(self):
        """å¼€å§‹æµå¼è¯†åˆ«"""
        self.is_running = True
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        process_thread = threading.Thread(target=self.process_audio)
        process_thread.start()
        
        print("\nğŸ¤ å¼€å§‹å½•éŸ³ (æŒ‰ Ctrl+C åœæ­¢)...")
        print("è¯´è¯æ—¶ä¼šå®æ—¶æ˜¾ç¤ºéŸ³é‡ï¼Œåœé¡¿åè‡ªåŠ¨è¯†åˆ«\n")
        
        # å¼€å§‹å½•éŸ³
        try:
            with sd.InputStream(
                samplerate=SAMPLE_RATE,
                channels=1,
                dtype='int16',
                blocksize=1024,  # æ¯ 1024 æ ·æœ¬è§¦å‘ä¸€æ¬¡å›è°ƒ (~64ms)
                callback=self.audio_callback
            ):
                while self.is_running:
                    time.sleep(0.1)
        except KeyboardInterrupt:
            print("\n\nåœæ­¢å½•éŸ³...")
        finally:
            self.is_running = False
            process_thread.join()
            
            # å¤„ç†å‰©ä½™éŸ³é¢‘
            if self.current_audio:
                self._recognize_and_output()


def test_file(model, audio_path: str):
    """æµ‹è¯•æ–‡ä»¶è¯†åˆ«"""
    print(f"\nè¯†åˆ«éŸ³é¢‘: {audio_path}")
    start_infer = time.time()
    
    results = model.transcribe(
        ["test_utterance"],
        [audio_path],
        {
            "use_gpu": 1,
            "beam_size": 3,
            "nbest": 1,
            "decode_max_len": 0,
            "softmax_smoothing": 1.0,
            "aed_length_penalty": 0.0,
            "eos_penalty": 1.0
        }
    )
    
    infer_time = time.time() - start_infer
    
    if results and len(results) > 0:
        text = results[0].get("text", "") if isinstance(results[0], dict) else str(results[0])
    else:
        text = ""
    
    print(f"\n{'='*50}")
    print(f"è¯†åˆ«ç»“æœ: {text}")
    print(f"æ¨ç†è€—æ—¶: {infer_time:.2f}s")
    print(f"{'='*50}")
    
    return text, infer_time


def main():
    print("=" * 60)
    print("FireRedASR-AED-L æµ‹è¯•è„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æ£€æŸ¥æ¨¡å‹
    if not os.path.exists(MODEL_DIR):
        print(f"\næ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_DIR}")
        print("è¯·å…ˆä¸‹è½½æ¨¡å‹:")
        print(f"huggingface-cli download FireRedTeam/FireRedASR-AED-L --local-dir {MODEL_DIR}")
        return
    
    # åŠ è½½æ¨¡å‹
    print(f"\n[DEBUG] å¼€å§‹åŠ è½½æ¨¡å‹...")
    print(f"[DEBUG] æ¨¡å‹ç›®å½•: {MODEL_DIR}")
    
    try:
        print("[DEBUG] æ­£åœ¨å¯¼å…¥ FireRedAsr...")
        from fireredasr.models.fireredasr import FireRedAsr
        print("[DEBUG] âœ“ FireRedAsr å¯¼å…¥æˆåŠŸ")
        
        print(f"\nåŠ è½½ FireRedASR-AED-L æ¨¡å‹...")
        start_load = time.time()
        
        # åˆ†æ­¥åŠ è½½ä»¥å®šä½é—®é¢˜
        import torch
        model_path = os.path.join(MODEL_DIR, "model.pth.tar")
        print(f"[DEBUG] æ¨¡å‹æ–‡ä»¶è·¯å¾„: {model_path}")
        print(f"[DEBUG] æ–‡ä»¶å­˜åœ¨: {os.path.exists(model_path)}")
        
        if os.path.exists(model_path):
            file_size = os.path.getsize(model_path) / (1024 * 1024 * 1024)
            print(f"[DEBUG] æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
        
        print("[DEBUG] æ­£åœ¨è°ƒç”¨ FireRedAsr.from_pretrained()...")
        print("[DEBUG] å¦‚æœå¡åœ¨è¿™é‡Œï¼Œè¯·ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆ...")
        sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
        
        model = FireRedAsr.from_pretrained("aed", MODEL_DIR)
        
        load_time = time.time() - start_load
        print(f"[DEBUG] âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}s)")
        sys.stdout.flush()
        
    except Exception as e:
        print(f"\n[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥!")
        print(f"[ERROR] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        print(f"[ERROR] å¼‚å¸¸ä¿¡æ¯: {e}")
        import traceback
        print("[ERROR] å®Œæ•´å †æ ˆ:")
        traceback.print_exc()
        sys.stdout.flush()
        return
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    while True:
        print("\né€‰æ‹©æµ‹è¯•æ¨¡å¼:")
        print("1. æµå¼å½•éŸ³æµ‹è¯• (è¯´å®Œä¸€å¥è¾“å‡ºä¸€å¥)")
        print("2. ä½¿ç”¨å·²æœ‰ WAV æ–‡ä»¶")
        print("q. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1/2/q): ").strip().lower()
        
        if choice == "1":
            recognizer = StreamingRecognizer(model)
            recognizer.start()
            break
        elif choice == "2":
            audio_path = input("è¯·è¾“å…¥ WAV æ–‡ä»¶è·¯å¾„: ").strip()
            if not audio_path:
                print("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            if not os.path.exists(audio_path):
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                continue
            test_file(model, audio_path)
            break
        elif choice == "q":
            print("é€€å‡ºæµ‹è¯•")
            return
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– q")
    
    print("\næµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
