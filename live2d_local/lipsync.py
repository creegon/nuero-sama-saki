# -*- coding: utf-8 -*-
"""
Lip Sync Module - å®æ—¶å£å‹åŒæ­¥
åŸºäºéŸ³é¢‘é¢‘è°±åˆ†ææ£€æµ‹å…ƒéŸ³ï¼Œé©±åŠ¨ Live2D å£å‹åŠ¨ç”»

å®ç°åŸç†:
1. å¯¹éŸ³é¢‘å—è¿›è¡Œ FFT é¢‘è°±åˆ†æ
2. æ ¹æ®ä½é¢‘/é«˜é¢‘èƒ½é‡æ¯”æ¨æ–­å…ƒéŸ³ç±»å‹ (A/I/U/E/O)
3. è¾“å‡ºå£å‹å‚æ•°ç”¨äºé©±åŠ¨ Live2D æ¨¡å‹
"""

import numpy as np
from typing import Tuple, Optional
from dataclasses import dataclass
from loguru import logger


@dataclass
class VowelShape:
    """å…ƒéŸ³å¯¹åº”çš„å£å‹å‚æ•°"""
    mouth_open: float    # å˜´å·´å¼ å¼€ç¨‹åº¦ (0-1)
    mouth_form: float    # å˜´å·´å½¢çŠ¶ (-1=åœ†å½¢, 0=ä¸­æ€§, 1=æ¨ªæ‹‰)
    
# å…ƒéŸ³å£å‹å®šä¹‰ (å‚è€ƒæ—¥è¯­å‘éŸ³)
VOWEL_SHAPES = {
    "A": VowelShape(mouth_open=1.0, mouth_form=0.0),    # ã‚ - å¤§å¼ å˜´
    "I": VowelShape(mouth_open=0.3, mouth_form=1.0),    # ã„ - æ¨ªæ‹‰å¾®å¼ 
    "U": VowelShape(mouth_open=0.4, mouth_form=-0.6),   # ã† - åœ†å½¢å¾®å¼ 
    "E": VowelShape(mouth_open=0.5, mouth_form=0.3),    # ãˆ - ä¸­ç­‰å¼ å¼€
    "O": VowelShape(mouth_open=0.7, mouth_form=-0.8),   # ãŠ - åœ†å½¢å¤§å¼ 
    "N": VowelShape(mouth_open=0.15, mouth_form=0.0),   # ã‚“ - é—­å˜´é¼»éŸ³
    "silence": VowelShape(mouth_open=0.0, mouth_form=0.0),  # é™éŸ³
}


class LipSyncAnalyzer:
    """
    å®æ—¶å£å‹åˆ†æå™¨
    åŸºäºéŸ³é¢‘é¢‘è°±åˆ†ææ¨æ–­å…ƒéŸ³
    """
    
    def __init__(self, sample_rate: int = 44100, smoothing: float = 0.3):
        """
        Args:
            sample_rate: é‡‡æ ·ç‡
            smoothing: å¹³æ»‘ç³»æ•° (0-1, è¶Šå¤§å˜åŒ–è¶Šå¹³æ»‘)
        """
        self.sample_rate = sample_rate
        self.smoothing = smoothing
        
        # å½“å‰çŠ¶æ€
        self._current_vowel = "silence"
        self._current_mouth_open = 0.0
        self._current_mouth_form = 0.0
        self._energy_history = []
        
        # é¢‘ç‡èŒƒå›´å®šä¹‰ (Hz)
        self.LOW_FREQ_RANGE = (100, 500)     # ä½é¢‘ (A, O ä¸»è¦åŒºåŸŸ)
        self.MID_FREQ_RANGE = (500, 1500)    # ä¸­é¢‘ (U, E ä¸»è¦åŒºåŸŸ)
        self.HIGH_FREQ_RANGE = (1500, 4000)  # é«˜é¢‘ (I ä¸»è¦åŒºåŸŸ)
        
        # èƒ½é‡é˜ˆå€¼
        self.SILENCE_THRESHOLD = 0.01
        self.VOWEL_THRESHOLD = 0.05
    
    def analyze(self, audio_chunk: np.ndarray) -> Tuple[str, float, float]:
        """
        åˆ†æéŸ³é¢‘å—ï¼Œè¿”å›å½“å‰å£å‹
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ® (float32, -1 åˆ° 1)
            
        Returns:
            (vowel, mouth_open, mouth_form)
            - vowel: å…ƒéŸ³ç±»å‹ (A/I/U/E/O/N/silence)
            - mouth_open: å˜´å·´å¼ å¼€ç¨‹åº¦ (0-1)
            - mouth_form: å˜´å·´å½¢çŠ¶ (-1 åˆ° 1)
        """
        if len(audio_chunk) == 0:
            return self._apply_smoothing("silence", 0.0, 0.0)
        
        # ç¡®ä¿æ˜¯ 1D æ•°ç»„
        if audio_chunk.ndim > 1:
            audio_chunk = audio_chunk.flatten()
        
        # è®¡ç®—æ€»èƒ½é‡ (RMS)
        rms = np.sqrt(np.mean(audio_chunk ** 2))
        
        # é™éŸ³æ£€æµ‹
        if rms < self.SILENCE_THRESHOLD:
            return self._apply_smoothing("silence", 0.0, 0.0)
        
        # FFT é¢‘è°±åˆ†æ
        fft = np.fft.rfft(audio_chunk)
        magnitude = np.abs(fft)
        freqs = np.fft.rfftfreq(len(audio_chunk), 1.0 / self.sample_rate)
        
        # è®¡ç®—å„é¢‘æ®µèƒ½é‡
        low_energy = self._get_band_energy(magnitude, freqs, *self.LOW_FREQ_RANGE)
        mid_energy = self._get_band_energy(magnitude, freqs, *self.MID_FREQ_RANGE)
        high_energy = self._get_band_energy(magnitude, freqs, *self.HIGH_FREQ_RANGE)
        
        total_energy = low_energy + mid_energy + high_energy + 1e-8
        
        # é¢‘ç‡èƒ½é‡æ¯”ä¾‹
        low_ratio = low_energy / total_energy
        mid_ratio = mid_energy / total_energy
        high_ratio = high_energy / total_energy
        
        # æ ¹æ®é¢‘è°±ç‰¹å¾æ¨æ–­å…ƒéŸ³
        vowel = self._classify_vowel(low_ratio, mid_ratio, high_ratio, rms)
        
        # è·å–å£å‹å‚æ•°
        shape = VOWEL_SHAPES.get(vowel, VOWEL_SHAPES["silence"])
        
        # æ ¹æ®èƒ½é‡è°ƒæ•´å¼ å˜´å¹…åº¦
        intensity = min(rms / 0.15, 1.0)  # å½’ä¸€åŒ–
        mouth_open = shape.mouth_open * intensity
        mouth_form = shape.mouth_form
        
        return self._apply_smoothing(vowel, mouth_open, mouth_form)
    
    def _get_band_energy(self, magnitude: np.ndarray, freqs: np.ndarray, 
                         low: float, high: float) -> float:
        """è®¡ç®—æŒ‡å®šé¢‘æ®µçš„èƒ½é‡"""
        mask = (freqs >= low) & (freqs <= high)
        if not np.any(mask):
            return 0.0
        return np.sum(magnitude[mask] ** 2)
    
    def _classify_vowel(self, low_ratio: float, mid_ratio: float, 
                        high_ratio: float, rms: float) -> str:
        """æ ¹æ®é¢‘è°±æ¯”ä¾‹åˆ†ç±»å…ƒéŸ³"""
        
        # I: é«˜é¢‘å ä¸»å¯¼
        if high_ratio > 0.4:
            return "I"
        
        # A: ä½é¢‘å¼ºï¼Œä¸­é«˜é¢‘ä¹Ÿæœ‰
        if low_ratio > 0.5 and mid_ratio > 0.2:
            return "A"
        
        # O: ä½é¢‘å ä¸»å¯¼ï¼Œé«˜é¢‘å¼±
        if low_ratio > 0.6 and high_ratio < 0.15:
            return "O"
        
        # U: ä¸­é¢‘å ä¸»å¯¼ï¼Œä½é¢‘ä¹Ÿæœ‰
        if mid_ratio > 0.4 and low_ratio > 0.3 and high_ratio < 0.2:
            return "U"
        
        # E: ä¸­é«˜é¢‘ï¼Œä½é¢‘é€‚ä¸­
        if mid_ratio > 0.35 and high_ratio > 0.2:
            return "E"
        
        # ä½èƒ½é‡æ—¶å¯èƒ½æ˜¯é¼»éŸ³æˆ–è¾…éŸ³
        if rms < self.VOWEL_THRESHOLD:
            return "N"
        
        # é»˜è®¤è¿”å› A
        return "A"
    
    def _apply_smoothing(self, vowel: str, mouth_open: float, 
                         mouth_form: float) -> Tuple[str, float, float]:
        """åº”ç”¨å¹³æ»‘è¿‡æ¸¡"""
        # å˜´å·´å¼ å¼€åº¦å¹³æ»‘
        self._current_mouth_open += (mouth_open - self._current_mouth_open) * (1 - self.smoothing)
        
        # å˜´å·´å½¢çŠ¶å¹³æ»‘
        self._current_mouth_form += (mouth_form - self._current_mouth_form) * (1 - self.smoothing)
        
        # å…ƒéŸ³ä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„
        self._current_vowel = vowel
        
        return (self._current_vowel, self._current_mouth_open, self._current_mouth_form)
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self._current_vowel = "silence"
        self._current_mouth_open = 0.0
        self._current_mouth_form = 0.0
        self._energy_history.clear()


class LipSyncController:
    """
    å£å‹æ§åˆ¶å™¨ - è¿æ¥åˆ†æå™¨ä¸ Live2D æ§åˆ¶å™¨
    """
    
    def __init__(self, live2d_controller=None, sample_rate: int = 44100):
        self.analyzer = LipSyncAnalyzer(sample_rate)
        self.controller = live2d_controller
        self.is_speaking = False
    
    def set_controller(self, controller):
        """è®¾ç½® Live2D æ§åˆ¶å™¨"""
        self.controller = controller
    
    def start_speaking(self):
        """å¼€å§‹è¯´è¯"""
        self.is_speaking = True
    
    def stop_speaking(self):
        """åœæ­¢è¯´è¯"""
        self.is_speaking = False
        self.analyzer.reset()
        if self.controller:
            self.controller.set_mouth_open(0.0)
    
    def process_audio(self, audio_chunk: np.ndarray):
        """å¤„ç†éŸ³é¢‘å—å¹¶æ›´æ–°å£å‹"""
        if not self.is_speaking:
            return
        
        vowel, mouth_open, mouth_form = self.analyzer.analyze(audio_chunk)
        
        if self.controller:
            self.controller.set_vowel(vowel, mouth_open, mouth_form)


# å…¨å±€å•ä¾‹
_lip_sync: Optional[LipSyncAnalyzer] = None


def get_lip_sync_analyzer(sample_rate: int = 44100) -> LipSyncAnalyzer:
    """è·å–å…¨å±€ LipSyncAnalyzer å®ä¾‹"""
    global _lip_sync
    if _lip_sync is None:
        _lip_sync = LipSyncAnalyzer(sample_rate)
    return _lip_sync


if __name__ == "__main__":
    # æµ‹è¯•
    import sounddevice as sd
    
    print("ğŸ¤ æµ‹è¯•å£å‹åˆ†æå™¨ - å¯¹ç€éº¦å…‹é£è¯´ A/I/U/E/O")
    print("æŒ‰ Ctrl+C é€€å‡º\n")
    
    analyzer = LipSyncAnalyzer(sample_rate=16000)
    
    def audio_callback(indata, frames, time, status):
        if status:
            print(f"Status: {status}")
        
        vowel, mouth_open, mouth_form = analyzer.analyze(indata[:, 0])
        
        # å¯è§†åŒ–
        bar_len = int(mouth_open * 20)
        bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
        form_indicator = "<" if mouth_form < -0.3 else (">" if mouth_form > 0.3 else "o")
        
        print(f"\r  {vowel:7} [{bar}] {form_indicator}", end="", flush=True)
    
    try:
        with sd.InputStream(channels=1, samplerate=16000, blocksize=512, 
                           callback=audio_callback):
            print("å½•éŸ³ä¸­...")
            while True:
                sd.sleep(100)
    except KeyboardInterrupt:
        print("\n\nç»“æŸ")
