# -*- coding: utf-8 -*-
"""
è®°å¿†ç®¡ç†å™¨
è´Ÿè´£è®°å¿†çš„é‡è¦æ€§è¯„åˆ†ã€å»é‡åˆå¹¶ã€è¡°å‡é—å¿˜ç­‰
"""

import time
from typing import Dict, List, Optional
from loguru import logger


class MemoryManager:
    """
    è®°å¿†ç®¡ç†å™¨
    
    æä¾›è®°å¿†çš„é«˜çº§ç®¡ç†åŠŸèƒ½ï¼š
    - é‡è¦æ€§è¯„åˆ†åŠ¨æ€è°ƒæ•´
    - ç›¸ä¼¼è®°å¿†å»é‡åˆå¹¶
    - é•¿æœŸæœªè®¿é—®è®°å¿†è¡°å‡
    """
    
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
    
    # ä¸´ç•Œå€¼
    PROMOTE_THRESHOLD = 2.5  # è¾¾åˆ°æ­¤å€¼è§¦å‘å‡çº§å®¡æ ¸
    DECAY_THRESHOLD = 0.2    # ä½äºæ­¤å€¼è§¦å‘åˆ é™¤å®¡æ ¸ï¼ˆæ›´æ¿€è¿›ï¼‰
    DELETE_COOLDOWN_HOURS = 24  # åˆ é™¤å®¡æ ¸å†·å´æœŸï¼ˆå°æ—¶ï¼‰
    
    # ğŸ”¥ BOOST é˜²åˆ·å‚æ•°
    BOOST_VALUE = 0.5           # å•æ¬¡ BOOST å¢é‡
    BOOST_COOLDOWN_HOURS = 2    # 2å°æ—¶å†…åªç®—1æ¬¡
    BOOST_DAILY_CAP = 1.0       # æ¯å¤©æ¯æ¡è®°å¿†æœ€å¤šæ¶¨ 1.0
    
    # ğŸ”¥ è¡°å‡å‚æ•°ï¼ˆæ›´æ¿€è¿›ï¼‰
    DECAY_DAYS_FACT = 5         # fact ç±»å‹ 5 å¤©åå¼€å§‹è¡°å‡
    DECAY_FACTOR_FACT = 0.85    # fact æ¯æ¬¡è¡°å‡ 15%
    DECAY_DAYS_EPISODE = 3      # episode ç±»å‹ 3 å¤©åå¼€å§‹è¡°å‡
    DECAY_FACTOR_EPISODE = 0.6  # episode æ¯æ¬¡è¡°å‡ 40%
    DELETE_DAYS_EPISODE = 7     # episode 7 å¤©åå¼ºåˆ¶åˆ é™¤
    
    def update_importance(self, doc_id: str, delta: float = 0.5, trigger_review: bool = True) -> bool:
        """
        æ›´æ–°è®°å¿†é‡è¦æ€§è¯„åˆ†
        
        Args:
            doc_id: æ–‡æ¡£ ID
            delta: è¯„åˆ†å˜åŒ–é‡ (æ­£æ•°å¢åŠ ï¼Œè´Ÿæ•°å‡å°‘)
            trigger_review: æ˜¯å¦åœ¨è¾¾åˆ°ä¸´ç•Œå€¼æ—¶è§¦å‘å®¡æ ¸
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            all_rows = self.kb._table.to_pandas()
            for idx, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    old_importance = metadata.get("importance", 1.0)
                    new_importance = max(0, old_importance + delta)
                    metadata["importance"] = new_importance
                    metadata["access_count"] = metadata.get("access_count", 0) + 1
                    metadata["last_access"] = time.time()
                    
                    # æ›´æ–°è®°å½•
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": row["text"],
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": row["vector"]
                    }])
                    
                    logger.debug(f"ğŸ“Š æ›´æ–°é‡è¦æ€§: [{doc_id}] {old_importance:.1f} -> {new_importance:.1f}")
                    
                    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å‡çº§å®¡æ ¸
                    if trigger_review and new_importance >= self.PROMOTE_THRESHOLD:
                        category = metadata.get("category", "fact")
                        promotion_rejected = metadata.get("promotion_rejected", False)
                        
                        # ğŸ”¥ å¦‚æœä¹‹å‰å·²ç»è¢«æ‹’ç»å‡çº§ï¼Œä¸å†è§¦å‘
                        if promotion_rejected:
                            logger.debug(f"â›” è·³è¿‡å‡çº§å®¡æ ¸ï¼ˆå·²è¢«æ·˜æ±°ï¼‰: [{doc_id}]")
                        elif category not in ["core", "system"]:
                            # å¼‚æ­¥è§¦å‘å‡çº§å®¡æ ¸
                            self._schedule_promotion_review({
                                "id": doc_id,
                                "text": row["text"],
                                "metadata": metadata
                            })
                    
                    return True
            return False
        except Exception as e:
            logger.error(f"æ›´æ–°é‡è¦æ€§å¤±è´¥: {e}")
            return False
    
    def boost_with_cooldown(self, doc_id: str) -> bool:
        """
        ğŸ”¥ å¸¦å†·å´å’Œæ¯æ—¥ä¸Šé™çš„ BOOST
        
        - 2å°æ—¶å†…å¤šæ¬¡ä½¿ç”¨åªç®—1æ¬¡
        - æ¯å¤©æ¯æ¡è®°å¿†æœ€å¤šæ¶¨ 1.0
        
        Returns:
            æ˜¯å¦æˆåŠŸæ‰§è¡Œ BOOST
        """
        try:
            from datetime import datetime
            
            all_rows = self.kb._table.to_pandas()
            for idx, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    now = time.time()
                    today = datetime.now().strftime("%Y-%m-%d")
                    
                    # æ£€æŸ¥å†·å´æœŸ
                    last_boost_time = metadata.get("last_boost_time", 0)
                    if now - last_boost_time < self.BOOST_COOLDOWN_HOURS * 3600:
                        logger.debug(f"â³ BOOST å†·å´ä¸­: [{doc_id}]")
                        return False
                    
                    # æ£€æŸ¥æ¯æ—¥ä¸Šé™
                    boost_date = metadata.get("boost_date", "")
                    if boost_date == today:
                        daily_boost = metadata.get("daily_boost_total", 0)
                        if daily_boost >= self.BOOST_DAILY_CAP:
                            logger.debug(f"ğŸ“Š BOOST è¾¾åˆ°æ¯æ—¥ä¸Šé™: [{doc_id}]")
                            return False
                    else:
                        # æ–°çš„ä¸€å¤©ï¼Œé‡ç½®è®¡æ•°
                        metadata["boost_date"] = today
                        metadata["daily_boost_total"] = 0
                    
                    # æ‰§è¡Œ BOOST
                    old_importance = metadata.get("importance", 1.0)
                    new_importance = old_importance + self.BOOST_VALUE
                    
                    metadata["importance"] = new_importance
                    metadata["last_boost_time"] = now
                    metadata["daily_boost_total"] = metadata.get("daily_boost_total", 0) + self.BOOST_VALUE
                    metadata["access_count"] = metadata.get("access_count", 0) + 1
                    metadata["last_access"] = now
                    
                    # æ›´æ–°è®°å½•
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": row["text"],
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": row["vector"]
                    }])
                    
                    logger.debug(f"ğŸ“Š BOOST: [{doc_id}] {old_importance:.1f} -> {new_importance:.1f}")
                    
                    # æ£€æŸ¥æ˜¯å¦è§¦å‘å‡çº§å®¡æ ¸
                    if new_importance >= self.PROMOTE_THRESHOLD:
                        category = metadata.get("category", "fact")
                        promotion_rejected = metadata.get("promotion_rejected", False)
                        
                        if not promotion_rejected and category not in ["core", "system"]:
                            self._schedule_promotion_review({
                                "id": doc_id,
                                "text": row["text"],
                                "metadata": metadata
                            })
                    
                    return True
            return False
        except Exception as e:
            logger.error(f"BOOST å¤±è´¥: {e}")
            return False
    
    def _schedule_promotion_review(self, memory: dict):
        """è°ƒåº¦å‡çº§å®¡æ ¸ï¼ˆå¼‚æ­¥ï¼‰"""
        import asyncio
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(self._run_promotion_review(memory))
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œè·³è¿‡
            logger.debug(f"â³ å‡çº§å®¡æ ¸å·²è·³è¿‡ï¼ˆæ— äº‹ä»¶å¾ªç¯ï¼‰: [{memory['id']}]")
    
    async def _run_promotion_review(self, memory: dict):
        """æ‰§è¡Œå‡çº§å®¡æ ¸"""
        try:
            from core.memory_reviewer import get_memory_reviewer
            reviewer = get_memory_reviewer()
            if reviewer:
                decision = await reviewer.review_for_promotion(memory)
                if decision == "PROMOTE":
                    self._promote_to_core(memory["id"])
                elif decision == "DELETE":
                    self.kb.delete(memory["id"])
                elif decision == "KEEP":
                    # ğŸ”¥ å‡çº§è¢«æ‹’ç»ï¼Œè®¾ç½®æ ‡è®°ï¼Œæ°¸ä¸å†è§¦å‘å‡çº§å®¡æ ¸
                    self._set_promotion_rejected(memory["id"])
        except Exception as e:
            logger.error(f"å‡çº§å®¡æ ¸æ‰§è¡Œå¤±è´¥: {e}")
    
    def _promote_to_core(self, doc_id: str):
        """å°†è®°å¿†å‡çº§ä¸º core"""
        try:
            all_rows = self.kb._table.to_pandas()
            for _, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    metadata["category"] = "core"
                    
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": row["text"],
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": row["vector"]
                    }])
                    
                    logger.info(f"â­ è®°å¿†å‡çº§ä¸ºæ ¸å¿ƒ: [{doc_id}]")
                    return True
            return False
        except Exception as e:
            logger.error(f"å‡çº§ä¸ºæ ¸å¿ƒå¤±è´¥: {e}")
            return False
    
    def update_text(self, doc_id: str, new_text: str) -> bool:
        """
        æ›´æ–°è®°å¿†çš„æ–‡æœ¬å†…å®¹ï¼ˆä¿ç•™ metadata å’Œé‡æ–°è®¡ç®— vectorï¼‰
        
        ğŸ”¥ ç‰¹åˆ«ç”¨äº core è®°å¿†çš„æ›´æ–°ï¼ˆcore ä¸å…è®¸åˆ é™¤ï¼Œä½†å…è®¸ä¿®æ”¹ï¼‰
        
        Args:
            doc_id: æ–‡æ¡£ ID
            new_text: æ–°çš„æ–‡æœ¬å†…å®¹
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            all_rows = self.kb._table.to_pandas()
            for _, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    
                    # é‡æ–°è®¡ç®—å‘é‡
                    new_vector = self.kb._embed(new_text)
                    
                    # æ›´æ–°è®°å½•
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": new_text,
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": new_vector
                    }])
                    
                    logger.info(f"ğŸ“ æ›´æ–°è®°å¿†å†…å®¹: [{doc_id}] â†’ {new_text[:50]}...")
                    return True
            
            logger.warning(f"âš ï¸ è®°å¿†ä¸å­˜åœ¨: [{doc_id}]")
            return False
        except Exception as e:
            logger.error(f"æ›´æ–°è®°å¿†å†…å®¹å¤±è´¥: {e}")
            return False
    
    def find_similar(self, text: str, threshold: float = 0.8) -> list:
        """
        æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†
        
        Args:
            text: è¦æ¯”è¾ƒçš„æ–‡æœ¬
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (1.0 = å®Œå…¨ç›¸åŒ, 0.8 = å¾ˆç›¸ä¼¼)
        
        Returns:
            ç›¸ä¼¼è®°å¿†åˆ—è¡¨ [{"id": ..., "text": ..., "similarity": ...}]
        """
        try:
            results = self.kb.search(text, n_results=5)
            
            similar = []
            for r in results:
                distance = r.get("distance", 2.0)
                similarity = max(0, 1 - distance / 2)
                if similarity >= threshold:
                    similar.append({
                        "id": r["id"],
                        "text": r.get("text", ""),
                        "similarity": similarity,
                        "metadata": r.get("metadata", {})
                    })
            
            return similar
        except Exception as e:
            logger.debug(f"æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†å¤±è´¥: {e}")
            return []
    
    def add_with_dedup(self, text: str, metadata: Dict = None, similarity_threshold: float = 0.85) -> str:
        """
        æ·»åŠ è®°å¿†ï¼ˆè‡ªåŠ¨å»é‡å’Œåˆå¹¶ï¼‰
        
        å¦‚æœå·²æœ‰éå¸¸ç›¸ä¼¼çš„è®°å¿†ï¼Œåˆ™å¢åŠ å…¶é‡è¦æ€§è€Œéé‡å¤æ·»åŠ 
        
        Returns:
            æ–‡æ¡£ IDï¼ˆæ–°å»ºæˆ–å·²å­˜åœ¨çš„ï¼‰
        """
        similar = self.find_similar(text, threshold=similarity_threshold)
        
        if similar:
            best_match = similar[0]
            self.update_importance(best_match["id"], delta=0.5)
            logger.info(f"ğŸ”— è®°å¿†åˆå¹¶: å¢å¼ºç°æœ‰è®°å¿† [{best_match['id']}] (ç›¸ä¼¼åº¦: {best_match['similarity']:.2f})")
            return best_match["id"]
        else:
            return self.kb.add(text, metadata)
    
    def decay_old_memories(self, days_threshold: int = 7, decay_factor: float = 0.9) -> int:
        """
        è¡°å‡é•¿æœŸæœªè®¿é—®çš„è®°å¿†
        
        ä¸‰å±‚æ¶æ„è¡°å‡è§„åˆ™ï¼š
        - system: æ°¸ä¸è¡°å‡ï¼ˆç³»ç»Ÿè®¾å®šï¼‰
        - core: æ°¸ä¸è¡°å‡ï¼ˆæ ¸å¿ƒäº‹å®ï¼Œimportance >= 3.0ï¼‰
        - episode: 7å¤©åå¿«é€Ÿè¡°å‡(0.8)ï¼Œ14å¤©ååˆ é™¤
        - fact: 7å¤©åæ­£å¸¸è¡°å‡(0.95)ï¼Œimportance < 0.3 æ—¶åˆ é™¤
        
        Returns:
            è¡°å‡çš„è®°å¿†æ•°é‡
        """
        try:
            all_rows = self.kb._table.to_pandas()
            now = time.time()
            
            # ğŸ”¥ ä½¿ç”¨ç±»å±æ€§å‚æ•°
            fact_threshold = self.DECAY_DAYS_FACT * 24 * 3600
            episode_threshold = self.DECAY_DAYS_EPISODE * 24 * 3600
            episode_delete_threshold = self.DELETE_DAYS_EPISODE * 24 * 3600
            
            decayed_count = 0
            deleted_count = 0
            deleted_memory_ids = []  # ç”¨äºçº§è”åˆ é™¤ä¸‰å…ƒç»„
            
            for _, row in all_rows.iterrows():
                metadata = self.kb._json.loads(row.get("metadata", "{}"))
                last_access = metadata.get("last_access", metadata.get("timestamp", 0))
                category = metadata.get("category", "fact")
                importance = metadata.get("importance", 1.0)
                
                # system/core æ°¸ä¸è¡°å‡
                if category in ["core", "system"]:
                    continue
                
                elapsed = now - last_access
                
                # episode ç±»å‹ï¼šå¿«é€Ÿè¡°å‡
                if category == "episode":
                    if elapsed > episode_delete_threshold:
                        self.kb.delete(row["id"])
                        deleted_memory_ids.append(row["id"])
                        logger.debug(f"ğŸ—‘ åˆ é™¤è¿‡æœŸæƒ…å¢ƒ: [{row['id']}]")
                        deleted_count += 1
                    elif elapsed > episode_threshold:
                        new_importance = importance * self.DECAY_FACTOR_EPISODE
                        if new_importance < self.DECAY_THRESHOLD:
                            self.kb.delete(row["id"])
                            deleted_memory_ids.append(row["id"])
                            logger.debug(f"ğŸ—‘ é—å¿˜æƒ…å¢ƒ: [{row['id']}]")
                            deleted_count += 1
                        else:
                            metadata["importance"] = new_importance
                            self._update_memory_metadata(row, metadata)
                            decayed_count += 1
                    continue
                
                # fact ç±»å‹ï¼šæ­£å¸¸è¡°å‡
                if elapsed > fact_threshold:
                    new_importance = importance * self.DECAY_FACTOR_FACT
                    metadata["importance"] = new_importance
                    self._update_memory_metadata(row, metadata)
                    decayed_count += 1
                    
                    # ğŸ”¥ ä½äºé˜ˆå€¼æ—¶è§¦å‘åˆ é™¤å®¡æ ¸
                    if new_importance < self.DECAY_THRESHOLD:
                        cooldown_until = metadata.get("delete_cooldown_until", 0)
                        if cooldown_until > time.time():
                            logger.debug(f"â›” è·³è¿‡åˆ é™¤å®¡æ ¸ï¼ˆå†·å´ä¸­ï¼‰: [{row['id']}]")
                        else:
                            self._schedule_decay_review({
                                "id": row["id"],
                                "text": row["text"],
                                "metadata": metadata
                            })
            
            # ğŸ”¥ çº§è”åˆ é™¤ä¸‰å…ƒç»„
            if deleted_memory_ids:
                try:
                    from .triple_store import get_triple_store
                    triple_store = get_triple_store()
                    for mid in deleted_memory_ids:
                        triple_store.remove_source(mid)
                except Exception as e:
                    logger.debug(f"çº§è”åˆ é™¤ä¸‰å…ƒç»„å¤±è´¥: {e}")
            
            if decayed_count > 0 or deleted_count > 0:
                logger.info(f"ğŸ§¹ è®°å¿†è¡°å‡: è¡°å‡ {decayed_count} æ¡ï¼Œåˆ é™¤ {deleted_count} æ¡")
            return decayed_count + deleted_count
            
        except Exception as e:
            logger.error(f"è®°å¿†è¡°å‡å¤±è´¥: {e}")
            return 0
    
    def _schedule_decay_review(self, memory: dict):
        """è°ƒåº¦è¡°å‡å®¡æ ¸ï¼ˆå¼‚æ­¥ï¼‰"""
        import asyncio
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(self._run_decay_review(memory))
        except RuntimeError:
            logger.debug(f"â³ è¡°å‡å®¡æ ¸å·²è·³è¿‡ï¼ˆæ— äº‹ä»¶å¾ªç¯ï¼‰: [{memory['id']}]")
    
    async def _run_decay_review(self, memory: dict):
        """æ‰§è¡Œè¡°å‡å®¡æ ¸"""
        try:
            from core.memory_reviewer import get_memory_reviewer
            reviewer = get_memory_reviewer()
            if reviewer:
                decision = await reviewer.review_for_decay(memory)
                if decision == "DELETE":
                    self.kb.delete(memory["id"])
                    logger.info(f"ğŸ—‘ å®¡æ ¸ååˆ é™¤: [{memory['id']}]")
                elif decision == "KEEP":
                    # ğŸ”¥ ä¿ç•™ï¼šé‡ç½® importance å¹¶è®¾ç½® 24h å†·å´æœŸ
                    self._reset_importance_with_cooldown(memory["id"], 0.5)
        except Exception as e:
            logger.error(f"è¡°å‡å®¡æ ¸æ‰§è¡Œå¤±è´¥: {e}")
    
    def _reset_importance(self, doc_id: str, new_importance: float):
        """é‡ç½®è®°å¿†çš„ importance"""
        try:
            all_rows = self.kb._table.to_pandas()
            for _, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    metadata["importance"] = new_importance
                    metadata["last_access"] = time.time()
                    
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": row["text"],
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": row["vector"]
                    }])
                    logger.debug(f"ğŸ“Š é‡ç½®é‡è¦æ€§: [{doc_id}] -> {new_importance}")
                    return True
            return False
        except Exception as e:
            logger.error(f"é‡ç½®é‡è¦æ€§å¤±è´¥: {e}")
            return False
    
    def _set_promotion_rejected(self, doc_id: str):
        """ğŸ”¥ è®¾ç½®å‡çº§è¢«æ‹’ç»æ ‡è®°ï¼ˆæ°¸ä¸å†è§¦å‘å‡çº§å®¡æ ¸ï¼‰"""
        try:
            all_rows = self.kb._table.to_pandas()
            for _, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    metadata["promotion_rejected"] = True
                    
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": row["text"],
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": row["vector"]
                    }])
                    logger.info(f"â›” è®¾ç½®å‡çº§æ·˜æ±°æ ‡è®°: [{doc_id}]")
                    return True
            return False
        except Exception as e:
            logger.error(f"è®¾ç½®æ·˜æ±°æ ‡è®°å¤±è´¥: {e}")
            return False
    
    def _reset_importance_with_cooldown(self, doc_id: str, new_importance: float):
        """ğŸ”¥ é‡ç½® importance å¹¶è®¾ç½®åˆ é™¤å®¡æ ¸å†·å´æœŸ"""
        try:
            all_rows = self.kb._table.to_pandas()
            for _, row in all_rows.iterrows():
                if row["id"] == doc_id:
                    metadata = self.kb._json.loads(row.get("metadata", "{}"))
                    metadata["importance"] = new_importance
                    metadata["last_access"] = time.time()
                    # ğŸ”¥ è®¾ç½®å†·å´æœŸ
                    cooldown_seconds = self.DELETE_COOLDOWN_HOURS * 3600
                    metadata["delete_cooldown_until"] = time.time() + cooldown_seconds
                    
                    self.kb._table.delete(f"id = '{doc_id}'")
                    self.kb._table.add([{
                        "id": doc_id,
                        "text": row["text"],
                        "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
                        "vector": row["vector"]
                    }])
                    logger.info(f"â³ è®¾ç½®åˆ é™¤å†·å´æœŸ: [{doc_id}] ({self.DELETE_COOLDOWN_HOURS}h)")
                    return True
            return False
        except Exception as e:
            logger.error(f"è®¾ç½®å†·å´æœŸå¤±è´¥: {e}")
            return False
    
    def _update_memory_metadata(self, row, metadata):
        """æ›´æ–°è®°å¿†çš„ metadata"""
        self.kb._table.delete(f"id = '{row['id']}'")
        self.kb._table.add([{
            "id": row["id"],
            "text": row["text"],
            "metadata": self.kb._json.dumps(metadata, ensure_ascii=False),
            "vector": row["vector"]
        }])


def create_memory_manager(knowledge_base) -> MemoryManager:
    """åˆ›å»ºè®°å¿†ç®¡ç†å™¨"""
    return MemoryManager(knowledge_base)
