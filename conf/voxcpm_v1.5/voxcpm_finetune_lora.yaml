pretrained_path: "openbmb/VoxCPM1.5"
train_manifest: "d:/neruo/datasets/sakiko_lora/train.jsonl"
val_manifest: ""

sample_rate: 44100
batch_size: 4 # Reduced for standard GPU memory, adjust if 4090 permits more
grad_accum_steps: 4
num_workers: 0 # Windows usually needs 0
num_iters: 2000
log_interval: 10
valid_interval: 1000
save_interval: 200

learning_rate: 0.0001
weight_decay: 0.01
warmup_steps: 100
max_steps: 3000
max_batch_tokens: 8192

save_path: "d:/neruo/checkpoints/sakiko_lora"
tensorboard: "d:/neruo/logs/sakiko_lora"

lambdas:
  loss/diff: 1.0
  loss/stop: 1.0

# LoRA configuration
lora:
  enable_lm: true
  enable_dit: true
  enable_proj: false

  r: 32
  alpha: 16
  dropout: 0.0

  target_modules_lm: ["q_proj", "v_proj", "k_proj", "o_proj"]
  target_modules_dit: ["q_proj", "v_proj", "k_proj", "o_proj"]
