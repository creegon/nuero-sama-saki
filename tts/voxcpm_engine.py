# -*- coding: utf-8 -*-
"""
VoxCPM TTS Engine - LoRA å¾®è°ƒç‰ˆ
ä½¿ç”¨ VoxCPM 1.5 + Sakiko LoRA è¿›è¡Œæœ¬åœ°è¯­éŸ³åˆæˆï¼Œæ”¯æŒæµå¼æ’­æ”¾
"""

import os
import sys
import time
import numpy as np
import sounddevice as sd
import torch
import soundfile as sf
import tempfile
import asyncio
from typing import Optional, Generator, List, Tuple
from loguru import logger
import config
from .emotion_data import get_emotion_audio

# (å·²ç§»é™¤ model_loader å¯¼å…¥)

class VoxCPMEngine:
    """VoxCPM TTS å¼•æ“ - æ”¯æŒæµå¼æ’­æ”¾"""
    
    def __init__(self):
        self._model = None
        self._sample_rate = 44100
        self._stream: Optional[sd.OutputStream] = None
        
        # RTF ç›‘æ§
        self._rtf_history: List[float] = []
        self._rtf_window = 5
        self._health_monitor = None
        
        self.output_device = config.AUDIO_OUTPUT_DEVICE
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self._model is not None:
            return True
            
        try:
            self._model = self._load_model()
            if self._model:
                self._sample_rate = self._model.tts_model.sample_rate
                return True
            return False
        except Exception as e:
            logger.error(f"VoxCPM åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def _load_model(self):
        """åŠ è½½ VoxCPM æ¨¡å‹ï¼ˆå†…è”è‡ªåŸ model_loader.pyï¼‰"""
        from voxcpm.core import VoxCPM
        
        model = None
        merged_weights_path = os.path.join(config.BASE_DIR, "checkpoints", "sakiko_merged", "tts_model_merged.pt")
        
        if os.path.exists(merged_weights_path):
            # ä¼˜å…ˆä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ (æ—  LoRA å¼€é”€ï¼ŒRTF < 1.0)
            logger.info("åŠ è½½ VoxCPM 1.5 (åˆå¹¶åæƒé‡)...")
            voxcpm = VoxCPM.from_pretrained(
                hf_model_id="openbmb/VoxCPM1.5",
                load_denoiser=False,
                optimize=False,
                lora_config=None,  # ä¸æ³¨å…¥ LoRA å±‚
                lora_weights_path=None,
            )
            
            # åŠ è½½åˆå¹¶åçš„æƒé‡
            logger.info(f"åŠ è½½åˆå¹¶åæƒé‡: {merged_weights_path}")
            merged_state = torch.load(merged_weights_path, map_location="cuda")
            model_state = voxcpm.tts_model.state_dict()
            
            for key in merged_state:
                if key in model_state:
                    model_state[key] = merged_state[key]
            
            voxcpm.tts_model.load_state_dict(model_state, strict=False)
            logger.info("åˆå¹¶æƒé‡åŠ è½½å®Œæˆã€‚")
            
        else:
            # å›é€€åˆ° LoRA æ¨¡å¼ (è¾ƒæ…¢ä½†å¯ç”¨)
            logger.warning(f"åˆå¹¶æƒé‡ä¸å­˜åœ¨: {merged_weights_path}ï¼Œå›é€€åˆ° LoRA æ¨¡å¼")
            from voxcpm.model.voxcpm import LoRAConfig
            
            lora_config = LoRAConfig(
                enable_lm=True,
                enable_dit=True,
                enable_proj=False,
                r=32,
                alpha=16,
                dropout=0.0,
                target_modules_lm=["q_proj", "v_proj", "k_proj", "o_proj"],
                target_modules_dit=["q_proj", "v_proj", "k_proj", "o_proj"],
            )
            
            voxcpm = VoxCPM.from_pretrained(
                hf_model_id="openbmb/VoxCPM1.5",
                load_denoiser=False,
                lora_config=lora_config,
                lora_weights_path=config.VOXCPM_LORA_PATH,
                optimize=False,
            )
            
            if os.path.exists(config.VOXCPM_LORA_PATH):
                voxcpm.tts_model.set_lora_enabled(True)
        
        # â­ FP16 ä¼˜åŒ–ï¼šå‡å°‘çº¦ 2GB æ˜¾å­˜ï¼Œé™ä½ RTF
        use_fp16 = getattr(config, "VOXCPM_USE_FP16", True)
        
        if use_fp16 and torch.cuda.is_available():
            try:
                # 1. è½¬æ¢æ¨¡å‹æƒé‡ä¸º FP16
                voxcpm.tts_model = voxcpm.tts_model.half()
                # 2. åŒæ­¥æ›´æ–° config.dtype
                voxcpm.tts_model.config.dtype = "float16"
                logger.info("âœ“ TTS æ¨¡å‹å·²è½¬æ¢ä¸º FP16ï¼ˆèŠ‚çœçº¦ 2GB æ˜¾å­˜ï¼ŒRTF æ›´ä½ï¼‰")
            except Exception as e:
                logger.warning(f"FP16 è½¬æ¢å¤±è´¥ï¼Œä¿æŒé»˜è®¤ç²¾åº¦: {e}")
        else:
            logger.info("âœ“ ä¿æŒé»˜è®¤ç²¾åº¦ (FP16 å·²ç¦ç”¨æˆ–æ—  CUDA)")
        
        logger.info(f"VoxCPM åŠ è½½å®Œæˆï¼Œé‡‡æ ·ç‡: {voxcpm.tts_model.sample_rate}Hz, è®¾å¤‡: cuda")
        return voxcpm

    def cleanup_cuda(self, aggressive: bool = False):
        """æ¸…ç† CUDA ç¼“å­˜ (ç¼“è§£å†…å­˜ç¢ç‰‡åŒ–)"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if aggressive:
                import gc
                gc.collect()

    def reload_model(self):
        """é‡æ–°åŠ è½½æ¨¡å‹ (ç”¨äº RTF é€€åŒ–æ—¶æ¢å¤)"""
        logger.warning("ğŸ”„ æ­£åœ¨é‡æ–°åŠ è½½ VoxCPM æ¨¡å‹...")
        
        # 1. å½»åº•é‡Šæ”¾æ—§æ¨¡å‹
        if self._model:
            del self._model
            self._model = None
            
        self.cleanup_cuda(aggressive=True)
        
        # 2. é‡æ–°åŠ è½½
        if self.initialize():
            logger.info("âœ“ æ¨¡å‹é‡è½½æˆåŠŸ")
            # é‡ç½® RTF å†å²
            self._rtf_history.clear()
        else:
            logger.error("âŒ æ¨¡å‹é‡è½½å¤±è´¥!")
            if self._health_monitor:
                self._health_monitor.report_issue("tts_reload_failed", "æ¨¡å‹é‡è½½å¤±è´¥")

    def set_health_monitor(self, health_monitor):
        """è®¾ç½®å¥åº·ç›‘æ§å™¨"""
        self._health_monitor = health_monitor

    def record_rtf(self, rtf: float):
        """è®°å½• RTF å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤"""
        self._rtf_history.append(rtf)
        if len(self._rtf_history) > self._rtf_window:
            self._rtf_history.pop(0)
            
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        avg_rtf = sum(self._rtf_history) / len(self._rtf_history)
        
        if self._health_monitor:
            self._health_monitor.record_rtf(rtf)
            
            # å¦‚æœå¹³å‡ RTF æŒç»­è¿‡é«˜ (> 1.5)ï¼Œä¸”ä¸åœ¨å¥åº·ç›‘æ§å™¨çš„å†·å´æœŸå†…
            # æ³¨æ„ï¼šHealthMonitor ä¼šå¤„ç†å†·å´é€»è¾‘ï¼Œè¿™é‡Œåªéœ€ä¸ŠæŠ¥
            if avg_rtf > 1.5:
                pass # äº¤ç»™ HealthMonitor åˆ¤æ–­æ˜¯å¦è§¦å‘å›è°ƒ
    
    def _calculate_dynamic_cfg(self, text: str) -> float:
        """æ ¹æ®æ–‡æœ¬é•¿åº¦åŠ¨æ€è®¡ç®—CFGå€¼
        
        çŸ­å¥ (<20å­—): é«˜CFG (2.5) - è¿½æ±‚æ¸…æ™°åº¦
        ä¸­å¥ (20-60å­—): ä¸­CFG (2.0) - å¹³è¡¡
        é•¿å¥ (>60å­—): ä½CFG (1.8) - æé«˜ç¨³å®šæ€§ï¼Œå‡å°‘é”™è¯¯ç´¯ç§¯
        """
        if not config.VOXCPM_USE_DYNAMIC_CFG:
            return config.VOXCPM_CFG_VALUE
        
        # å»é™¤æ ‡ç‚¹ç¬¦å·è®¡ç®—çº¯æ–‡æœ¬é•¿åº¦
        import re
        text_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€,.!?â€¦\s]', '', text)
        length = len(text_clean)
        
        if length < 20:
            cfg = config.VOXCPM_CFG_SHORT
            logger.debug(f"ğŸ“ çŸ­å¥ ({length}å­—) â†’ CFG={cfg}")
        elif length <= 60:
            cfg = config.VOXCPM_CFG_MEDIUM
            logger.debug(f"ğŸ“ ä¸­å¥ ({length}å­—) â†’ CFG={cfg}")
        else:
            cfg = config.VOXCPM_CFG_LONG
            logger.debug(f"ğŸ“ é•¿å¥ ({length}å­—) â†’ CFG={cfg}")
        
        return cfg
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬ä»¥é¿å…TTSé—®é¢˜"""
        import re
        
        # 1. å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text.strip())
        
        # 2. é™åˆ¶é•¿åº¦ï¼ˆè¿‡é•¿æ–‡æœ¬å®¹æ˜“å‡ºé—®é¢˜ï¼‰
        if len(text) > 150:
            logger.warning(f"æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—)ï¼Œæˆªæ–­åˆ°150å­—")
            text = text[:150]
        
        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½å¯¼è‡´å¼‚å¸¸ï¼‰
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€,.!?â€¦\s]', '', text)
        
        # 4. çŸ­æ–‡æœ¬å¤„ç†ï¼ˆä¼˜åŒ–ï¼‰
        # å»é™¤æ ‡ç‚¹ç¬¦å·è®¡ç®—çº¯æ–‡æœ¬é•¿åº¦
        text_no_punct = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€,.!?â€¦\s]', '', text)
        if len(text_no_punct) < 3:
            logger.warning(f"æ–‡æœ¬è¿‡çŸ­ ({len(text_no_punct)} å­—): '{text}'ï¼Œæ·»åŠ åœé¡¿ç¬¦")
            # æ·»åŠ åœé¡¿è®©TTSæ›´ç¨³å®š
            if not text.endswith('â€¦'):
                text = text.rstrip(',.!?ï¼Œã€‚ï¼ï¼Ÿã€') + 'â€¦'
        
        # 5. ğŸ”¥ ç»“å°¾å¡«å……ï¼šé˜²æ­¢éŸ³é¢‘è¢«æˆªæ–­
        # ç¡®ä¿å¥å­ç»“å°¾æœ‰é€‚å½“çš„æ ‡ç‚¹ï¼Œç»™VoxCPMè¶³å¤Ÿçš„â€œç»“æŸä¿¡å·â€
        if text and len(text_no_punct) >= 3:  # åªå¤„ç†æ­£å¸¸é•¿åº¦çš„æ–‡æœ¬
            ending_puncts = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'â€¦']
            has_ending = any(text.endswith(p) for p in ending_puncts)
            
            if not has_ending:
                # æ²¡æœ‰ç»“æŸæ ‡ç‚¹ï¼Œæ·»åŠ çœç•¥å·ä½œä¸ºç¼“å†²
                text = text.rstrip() + 'â€¦'
                logger.debug(f"ğŸ”§ ç»“å°¾å¡«å……: æ·»åŠ çœç•¥å·")
        
        # 6. æœ€ç»ˆç¡®ä¿éç©º
        if not text or len(text) == 0:
            logger.warning("æ–‡æœ¬ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬")
            text = "å—¯â€¦"
        
        return text

 
    @property
    def sample_rate(self) -> int:
        return self._sample_rate

    def synthesize_streaming(
        self,
        text: str,
        prompt_wav_path: Optional[str] = None,
        prompt_text: Optional[str] = None,
        cfg_value: float = config.VOXCPM_CFG_VALUE,
        inference_timesteps: int = config.VOXCPM_INFERENCE_STEPS,
        emotion: Optional[str] = None,
    ) -> Generator[np.ndarray, None, None]:
        """
        æµå¼ç”Ÿæˆè¯­éŸ³
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (å¯é€‰)
            prompt_text: å‚è€ƒéŸ³é¢‘å¯¹åº”æ–‡æœ¬ (å¯é€‰)
            cfg_value: CFG å¼ºåº¦
            inference_timesteps: æ¨ç†æ­¥æ•°
            emotion: æƒ…æ„Ÿæ ‡ç­¾
        """
        if not self.initialize():
            logger.error("TTS æ¨¡å‹æœªåˆå§‹åŒ–")
            yield np.zeros(1024, dtype=np.float32)
            return

        # 1. è‡ªåŠ¨é€‰æ‹©æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘ (å¦‚æœæœªæŒ‡å®š prompt)
        if emotion and not prompt_wav_path and config.VOXCPM_USE_EMOTION_REF:
            prompt_wav_path, prompt_text = get_emotion_audio(emotion)
            if prompt_wav_path:
                logger.debug(f"ğŸ­ ä½¿ç”¨æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘ [{emotion}]: {os.path.basename(prompt_wav_path)}")

        # 2. é»˜è®¤å‚è€ƒéŸ³é¢‘
        if not prompt_wav_path:
            prompt_wav_path = config.VOXCPM_PROMPT_WAV
            prompt_text = config.VOXCPM_PROMPT_TEXT

        # æ–‡æœ¬é¢„å¤„ç†
        text = self._preprocess_text(text)
        
        # ğŸ”¥ åŠ¨æ€CFGï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦è‡ªåŠ¨è°ƒæ•´
        if cfg_value == config.VOXCPM_CFG_VALUE:  # åªåœ¨ä½¿ç”¨é»˜è®¤å€¼æ—¶åŠ¨æ€è°ƒæ•´
            cfg_value = self._calculate_dynamic_cfg(text)
        
        logger.info(f"ğŸµ TTS åˆæˆ: '{text}' (æƒ…æ„Ÿ: {emotion or 'default'}, CFG: {cfg_value})")
        start_time = time.time()
        
        try:
            # ğŸ”¥ ç§»é™¤æ¯æ¬¡åˆæˆå‰çš„ cleanup_cuda()
            # torch.cuda.empty_cache() ä¼šå¯¼è‡´ GPU åŒæ­¥ï¼Œå¢åŠ é¦–åŒ…å»¶è¿Ÿ
            # åªåœ¨ OOM å¼‚å¸¸æ—¶æ¸…ç†å³å¯
            
            # 3. æµå¼æ¨ç†
            wav_generator = self._model.generate_streaming(
                text=text,
                prompt_wav_path=prompt_wav_path,
                prompt_text=prompt_text,
                cfg_value=cfg_value,
                inference_timesteps=inference_timesteps,
                max_len=2048
                # æ³¨ï¼šretry_badcase åœ¨ streaming æ¨¡å¼ä¸‹ä¸æ”¯æŒï¼Œå·²ç§»é™¤
            )

            
            # 4. ç”Ÿæˆæ‰€æœ‰å—
            full_wav_chunks = []
            first_chunk_Time = 0
            
            for i, chunk in enumerate(wav_generator):
                if i == 0:
                    first_chunk_Time = time.time() - start_time
                    logger.debug(f"âš¡ é¦–åŒ…å»¶è¿Ÿ: {first_chunk_Time*1000:.1f}ms")
                
                # float32 chunk
                yield chunk
                full_wav_chunks.append(chunk)

            # 5. è®¡ç®— RTF å¹¶éªŒè¯éŸ³é¢‘è´¨é‡
            if full_wav_chunks:
                full_wav = np.concatenate(full_wav_chunks)
                audio_duration = len(full_wav) / self._sample_rate
                total_time = time.time() - start_time
                rtf = total_time / audio_duration if audio_duration > 0 else 0
                
                # éŸ³é¢‘è´¨é‡éªŒè¯
                from .audio_validator import AudioValidator
                is_valid, reason = AudioValidator.validate(full_wav, self._sample_rate)
                if not is_valid:
                    logger.warning(f"âš ï¸ éŸ³é¢‘è´¨é‡å¼‚å¸¸: {reason}")
                
                logger.info(f"âœ“ åˆæˆå®Œæˆ (æ—¶é•¿: {audio_duration:.1f}s, RTF: {rtf:.2f})")
                self.record_rtf(rtf)
                
        except Exception as e:
            logger.error(f"TTS åˆæˆå¤±è´¥: {e}")
            if "CUDA out of memory" in str(e):
                self.cleanup_cuda(aggressive=True)

    def synthesize_and_play(
        self,
        text: str,
        prompt_wav_path: Optional[str] = None,
        prompt_text: Optional[str] = None,
        cfg_value: float = config.VOXCPM_CFG_VALUE,
        inference_timesteps: int = config.VOXCPM_INFERENCE_STEPS,
        on_first_chunk: Optional[callable] = None,
        emotion: Optional[str] = None,
    ):
        """è¾¹ç”Ÿæˆè¾¹æ’­æ”¾"""
        try:
            # åˆå§‹åŒ–æ’­æ”¾æµ
            if self._stream is None or not self._stream.active:
                self._stream = sd.OutputStream(
                    samplerate=self._sample_rate,
                    channels=1,
                    dtype=np.float32,
                    device=self.output_device
                )
                self._stream.start()

            for i, chunk in enumerate(self.synthesize_streaming(
                text, prompt_wav_path, prompt_text, cfg_value, inference_timesteps, emotion
            )):
                if i == 0 and on_first_chunk:
                    on_first_chunk()
                
                self._stream.write(chunk)
                
            # è¿™é‡Œçš„æµä¸å…³é—­ï¼Œä¿æŒå¤ç”¨ï¼Œç›´åˆ°å¯¹è±¡é”€æ¯æˆ–é”™è¯¯

        except Exception as e:
            logger.error(f"æ’­æ”¾å¤±è´¥: {e}")
            if self._stream:
                self._stream.close()
                self._stream = None

    def synthesize(
        self,
        text: str,
        output_path: Optional[str] = None,
        emotion: Optional[str] = None,
    ) -> Optional[np.ndarray]:
        """ç”Ÿæˆè¯­éŸ³å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
        chunks = []
        for chunk in self.synthesize_streaming(text, emotion=emotion):
            chunks.append(chunk)
            
        if not chunks:
            return None
            
        full_wav = np.concatenate(chunks)
        
        if output_path:
            sf.write(output_path, full_wav, self._sample_rate)
            logger.info(f"å·²ä¿å­˜éŸ³é¢‘: {output_path}")
            
        return full_wav


# å…¨å±€å•ä¾‹
_engine: Optional[VoxCPMEngine] = None


def get_voxcpm_engine() -> VoxCPMEngine:
    """è·å–å…¨å±€ VoxCPM å¼•æ“å®ä¾‹"""
    global _engine
    if _engine is None:
        _engine = VoxCPMEngine()
    return _engine


if __name__ == "__main__":
    engine = VoxCPMEngine()
    
    if engine.initialize():
        test_texts = [
            "ä½ å¥½å‘€ï¼Œè§åˆ°ä½ çœŸå¼€å¿ƒï¼",
            "åˆ«ç”¨é‚£ç§çœ¼ç¥çœ‹æˆ‘ï¼Œæˆ‘ä¸éœ€è¦åŒæƒ…ã€‚",
        ]
        
        for text in test_texts:
            print(f"\næ’­æ”¾: {text}")
            engine.synthesize_and_play(text)
    else:
        print("VoxCPM åˆå§‹åŒ–å¤±è´¥")
