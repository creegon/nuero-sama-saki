# -*- coding: utf-8 -*-
"""
LLM Client Module
OpenAI æ ¼å¼ API å®¢æˆ·ç«¯ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’ŒéŸ³é¢‘è¾“å…¥
"""

import httpx
import base64
from typing import AsyncGenerator, Optional, Dict, Any, Union
from loguru import logger
import json
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config


class LLMClient:
    """LLM API å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        api_base: str = config.LLM_API_BASE,
        api_key: str = config.LLM_API_KEY,
        model: str = config.LLM_MODEL,
        timeout: float = 60.0
    ):
        self.api_base = api_base.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.timeout = timeout
        
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
    
    async def chat_stream(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        temperature: float = 1.2,
        max_tokens: int = 2048
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯
        
        Args:
            messages: å¯¹è¯å†å²
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            
        Yields:
            æµå¼è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
        """
        # æ„å»ºè¯·æ±‚æ¶ˆæ¯
        request_messages = []
        if system_prompt:
            request_messages.append({"role": "system", "content": system_prompt})
        request_messages.extend(messages)
        
        payload = {
            "model": self.model,
            "messages": request_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True
        }
        
        url = f"{self.api_base}/chat/completions"
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            async with client.stream(
                "POST",
                url,
                json=payload,
                headers=self.headers
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    logger.error(f"LLM API é”™è¯¯: {response.status_code} - {error_text}")
                    raise Exception(f"LLM API é”™è¯¯: {response.status_code}")
                
                async for line in response.aiter_lines():
                    if not line:
                        continue
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                delta = chunk["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield content
                        except json.JSONDecodeError:
                            continue
    
    async def chat_with_audio_stream(
        self,
        audio_data: bytes,
        audio_format: str = "wav",
        system_prompt: Optional[str] = None,
        conversation_history: Optional[list] = None,
        temperature: float = 1.2,
        max_tokens: int = 2048
    ) -> AsyncGenerator[str, None]:
        """
        è¯­éŸ³ç›´æ¥è¾“å…¥çš„æµå¼å¯¹è¯ (è·³è¿‡ STT)
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘å­—èŠ‚æ•°æ®
            audio_format: éŸ³é¢‘æ ¼å¼ (wav, mp3, etc.)
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            
        Yields:
            æµå¼è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
        """
        # Base64 ç¼–ç éŸ³é¢‘
        base64_audio = base64.b64encode(audio_data).decode("utf-8")
        
        # æ„å»ºè¯·æ±‚æ¶ˆæ¯
        request_messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            request_messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            request_messages.extend(conversation_history)
        
        # æ·»åŠ å½“å‰éŸ³é¢‘ä½œä¸ºç”¨æˆ·è¾“å…¥
        user_content = [
            {
                "type": "input_audio",
                "input_audio": {
                    "data": base64_audio,
                    "format": audio_format
                }
            }
        ]
        request_messages.append({"role": "user", "content": user_content})
        
        payload = {
            "model": self.model,
            "messages": request_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True
        }
        
        url = f"{self.api_base}/chat/completions"
        
        logger.debug(f"ğŸ¤ å‘é€éŸ³é¢‘åˆ° LLM ({len(audio_data)//1024}KB)")
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            async with client.stream(
                "POST",
                url,
                json=payload,
                headers=self.headers
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    logger.error(f"LLM API é”™è¯¯: {response.status_code} - {error_text}")
                    raise Exception(f"LLM API é”™è¯¯: {response.status_code}")
                
                async for line in response.aiter_lines():
                    if not line:
                        continue
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                delta = chunk["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield content
                        except json.JSONDecodeError:
                            continue
    
    async def chat(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        temperature: float = 1.2,
        max_tokens: int = 2048
    ) -> str:
        """
        éæµå¼å¯¹è¯ï¼ˆæ”¶é›†å®Œæ•´å“åº”ï¼‰
        """
        full_response = ""
        async for chunk in self.chat_stream(messages, system_prompt, temperature, max_tokens):
            full_response += chunk
        return full_response
    
    async def chat_with_audio(
        self,
        audio_data: bytes,
        audio_format: str = "wav",
        system_prompt: Optional[str] = None,
        conversation_history: Optional[list] = None,
        temperature: float = 1.2,
        max_tokens: int = 2048
    ) -> str:
        """
        è¯­éŸ³ç›´æ¥è¾“å…¥çš„éæµå¼å¯¹è¯
        """
        full_response = ""
        async for chunk in self.chat_with_audio_stream(
            audio_data, audio_format, system_prompt, 
            conversation_history, temperature, max_tokens
        ):
            full_response += chunk
        return full_response


# å…¨å±€å•ä¾‹
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """è·å–å…¨å±€ LLMClient å®ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client


if __name__ == "__main__":
    import asyncio
    
    async def test():
        client = LLMClient()
        messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}]
        
        print("æµ‹è¯•æµå¼è¾“å‡º:")
        async for chunk in client.chat_stream(messages, system_prompt=config.SYSTEM_PROMPT):
            print(chunk, end="", flush=True)
        print("\n")
    
    asyncio.run(test())
