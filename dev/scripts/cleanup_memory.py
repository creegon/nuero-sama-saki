# -*- coding: utf-8 -*-
"""
æ˜¾å­˜/å†…å­˜æ¸…ç†å·¥å…·
å®šæœŸæ¸…ç† GPU æ˜¾å­˜ç¢ç‰‡å’Œ Python å†…å­˜ï¼Œé¿å… RTF è¶Šè·‘è¶Šä½

ç”¨æ³•:
    # ä½œä¸ºæ¨¡å—å¯¼å…¥ï¼Œå®šæœŸè°ƒç”¨
    from scripts.cleanup_memory import cleanup_memory, cleanup_cuda
    
    # æˆ–è€…ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿è¡Œ
    python scripts/cleanup_memory.py
"""

import gc
import sys
from loguru import logger

def cleanup_memory():
    """
    æ¸…ç† Python å†…å­˜
    - å¼ºåˆ¶åƒåœ¾å›æ”¶
    - æ¸…ç†å¾ªç¯å¼•ç”¨
    """
    # å¤šæ¬¡ GC ä»¥ç¡®ä¿æ¸…ç†å¹²å‡€
    gc.collect()
    gc.collect()
    gc.collect()
    
    logger.debug("ğŸ§¹ Python å†…å­˜å·²æ¸…ç†")


def cleanup_cuda(aggressive: bool = False):
    """
    æ¸…ç† CUDA æ˜¾å­˜
    - æ¸…ç©ºç¼“å­˜
    - é‡ç½®å³°å€¼ç»Ÿè®¡
    - aggressiveæ¨¡å¼ï¼šå¤šè½®æ¸…ç†+ç¢ç‰‡æ•´ç†
    """
    try:
        import torch
        if torch.cuda.is_available():
            if aggressive:
                # æ¿€è¿›æ¨¡å¼ï¼šå¤šè½®æ¸…ç†
                for i in range(3):
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    if i < 2:
                        import time
                        time.sleep(0.1)

                logger.info(f"ğŸ® CUDA æ¿€è¿›æ¸…ç†å®Œæˆ (3è½®)")
            else:
                # åŒæ­¥ CUDA æ“ä½œ
                torch.cuda.synchronize()
                # æ¸…ç©ºç¼“å­˜
                torch.cuda.empty_cache()

            # é‡ç½®å³°å€¼ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()

            # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3

            logger.debug(f"ğŸ® CUDA æ˜¾å­˜å·²æ¸…ç† (å·²åˆ†é…: {allocated:.2f}GB, å·²ä¿ç•™: {reserved:.2f}GB)")
            return True
    except ImportError:
        pass
    except Exception as e:
        logger.warning(f"CUDA æ¸…ç†å¤±è´¥: {e}")
    return False


def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆTTSè¾“å‡ºç­‰ï¼‰"""
    import shutil
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    import config

    try:
        # æ¸…ç†TTSè¾“å‡ºç›®å½•
        tts_dir = config.TTS_OUTPUT_DIR
        if os.path.exists(tts_dir):
            file_count = 0
            for filename in os.listdir(tts_dir):
                file_path = os.path.join(tts_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.unlink(file_path)
                        file_count += 1
                except Exception as e:
                    logger.debug(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path}, {e}")

            if file_count > 0:
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶TTSæ–‡ä»¶: {file_count}ä¸ª")

        # æ¸…ç†debugéŸ³é¢‘ï¼ˆå¯é€‰ï¼Œä¿ç•™æœ€è¿‘çš„ï¼‰
        debug_dir = os.path.join(config.BASE_DIR, "debug_audio")
        if os.path.exists(debug_dir):
            files = [(os.path.join(debug_dir, f), os.path.getmtime(os.path.join(debug_dir, f)))
                     for f in os.listdir(debug_dir) if os.path.isfile(os.path.join(debug_dir, f))]
            files.sort(key=lambda x: x[1], reverse=True)

            # ä¿ç•™æœ€è¿‘50ä¸ªï¼Œåˆ é™¤å…¶ä»–
            if len(files) > 50:
                for file_path, _ in files[50:]:
                    try:
                        os.unlink(file_path)
                    except Exception:
                        pass
                logger.debug(f"ğŸ—‘ï¸ æ¸…ç†æ—§debugéŸ³é¢‘: {len(files) - 50}ä¸ª")

    except Exception as e:
        logger.warning(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")


def cleanup_all(aggressive: bool = False):
    """
    æ¸…ç†æ‰€æœ‰å†…å­˜ (Python + CUDA + ä¸´æ—¶æ–‡ä»¶)

    Args:
        aggressive: æ˜¯å¦ä½¿ç”¨æ¿€è¿›æ¨¡å¼ï¼ˆæ›´å½»åº•ä½†æ›´æ…¢ï¼‰
    """
    cleanup_memory()
    cleanup_cuda(aggressive=aggressive)
    if aggressive:
        cleanup_temp_files()


def get_memory_stats() -> dict:
    """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "python": {},
        "cuda": {}
    }
    
    # Python å†…å­˜
    import psutil
    process = psutil.Process()
    mem_info = process.memory_info()
    stats["python"]["rss_mb"] = mem_info.rss / 1024**2
    stats["python"]["vms_mb"] = mem_info.vms / 1024**2
    
    # CUDA å†…å­˜
    try:
        import torch
        if torch.cuda.is_available():
            stats["cuda"]["allocated_gb"] = torch.cuda.memory_allocated() / 1024**3
            stats["cuda"]["reserved_gb"] = torch.cuda.memory_reserved() / 1024**3
            stats["cuda"]["max_allocated_gb"] = torch.cuda.max_memory_allocated() / 1024**3
    except ImportError:
        pass
    
    return stats


def start_periodic_cleanup(interval_seconds: int = 300, aggressive: bool = False):
    """
    å¯åŠ¨å®šæœŸæ¸…ç†

    Args:
        interval_seconds: æ¸…ç†é—´éš” (ç§’)ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
        aggressive: æ˜¯å¦ä½¿ç”¨æ¿€è¿›æ¸…ç†æ¨¡å¼
    """
    import threading
    import time

    def cleanup_loop():
        cleanup_count = 0
        while True:
            time.sleep(interval_seconds)
            cleanup_count += 1

            # æ¯3æ¬¡åšä¸€æ¬¡æ¿€è¿›æ¸…ç†
            use_aggressive = aggressive or (cleanup_count % 3 == 0)
            cleanup_all(aggressive=use_aggressive)

            mode = "æ¿€è¿›" if use_aggressive else "å¸¸è§„"
            logger.info(f"â° å®šæœŸå†…å­˜æ¸…ç†å®Œæˆ ({mode}æ¨¡å¼, é—´éš”: {interval_seconds}s)")

    thread = threading.Thread(target=cleanup_loop, daemon=True)
    thread.start()
    logger.info(f"ğŸ”„ å·²å¯åŠ¨å®šæœŸå†…å­˜æ¸…ç† (é—´éš”: {interval_seconds}s)")
    return thread


if __name__ == "__main__":
    # æµ‹è¯•
    print("ğŸ§¹ æ˜¾å­˜/å†…å­˜æ¸…ç†å·¥å…·\n")
    
    # æ˜¾ç¤ºæ¸…ç†å‰çŠ¶æ€
    print("æ¸…ç†å‰:")
    stats = get_memory_stats()
    print(f"  Python: RSS={stats['python']['rss_mb']:.1f}MB")
    if stats["cuda"]:
        print(f"  CUDA: {stats['cuda']['allocated_gb']:.2f}GB / {stats['cuda']['reserved_gb']:.2f}GB")
    
    # æ‰§è¡Œæ¸…ç†
    print("\næ‰§è¡Œæ¸…ç†...")
    cleanup_all()
    
    # æ˜¾ç¤ºæ¸…ç†åçŠ¶æ€
    print("\næ¸…ç†å:")
    stats = get_memory_stats()
    print(f"  Python: RSS={stats['python']['rss_mb']:.1f}MB")
    if stats["cuda"]:
        print(f"  CUDA: {stats['cuda']['allocated_gb']:.2f}GB / {stats['cuda']['reserved_gb']:.2f}GB")
    
    print("\nâœ… æ¸…ç†å®Œæˆ!")
